{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a0f62c",
   "metadata": {},
   "source": [
    "# DATA2060 Final Project\n",
    "\n",
    "Model: **CART for classification**\\\n",
    "Github repo: https://github.com/mindyxu0125/Data2060_Human_not_learning.git\n",
    "\n",
    "Team Members:\n",
    "- Muxin Fu\n",
    "- Yixiao Zhang\n",
    "- Jingmin Xu\n",
    "- Mingrui Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6573eaa",
   "metadata": {},
   "source": [
    "# Part 1: Overview of CART for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a5121",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "### 0.1 Overview  \n",
    "The Classification and Regression Tree (CART) algorithm is a nonparametric supervised learning method that builds a binary decision tree for classification tasks. \n",
    "\n",
    "At each step, the algorithm selects a feature and threshold that create two child nodes with lower class impurity, using criteria such as Gini impurity or entropy. Through this recursive partitioning, CART represents the classifier as a set of piecewise-constant regions, where each leaf corresponds to a predicted class label. Because the sequence of splits directly mirrors the decision-making process, CART offers a transparent and intuitive model structure.\n",
    "\n",
    "### 0.2 Advantages  \n",
    "CART offers several notable strengths that contribute to its widespread use as a baseline classifier. \n",
    "\n",
    "- The key difference is that CART opens the window for allowing **splits on continunous features**, where it applies the threshold-splitting. Therefore, the model is highly interpretable: each internal node corresponds to a clear “if–then” condition based on a single feature, allowing the entire decision path to be easily traced and communicated. This transparency is particularly valuable in settings where model explanations are required.  \n",
    "\n",
    "- Second, CART is able to **capture nonlinear relationships** and feature interactions without relying on explicit transformations or parametric assumptions. Its recursive splitting procedure enables the model to adapt flexibly to irregular or complex decision boundaries, providing expressive power beyond that of linear models.  \n",
    "\n",
    "- Moreover, CART requires **minimal preprocessing**. It can accommodate both numerical and categorical variables, is robust to monotonic feature scaling, and implicitly performs feature selection by choosing splits only on informative variables. These characteristics make CART convenient to implement and reliable across a wide range of practical applications.\n",
    "\n",
    "### 0.3 Disadvantages  \n",
    "Despite its advantages, CART also presents several limitations that must be considered. \n",
    "\n",
    "- Most importantly, the model is prone to overfitting when allowed to grow without constraints. As emphasized in the bias–complexity trade-off discussed in the course reading, increasing model flexibility reduces approximation error but raises estimation error, causing deep, unpruned trees to exhibit high variance and poor generalization.  \n",
    "\n",
    "- CART also tends to be unstable: small perturbations in the training data can alter early splits, resulting in substantially different tree structures. This sensitivity undermines the model’s reliability, especially in contexts requiring stable predictions.  \n",
    "\n",
    "- Finally, because CART relies exclusively on axis-aligned splits, it may need many successive partitions to approximate diagonal or curved decision boundaries, leading to unnecessarily deep and complex trees. These shortcomings motivate the use of pruning techniques and more advanced ensemble methods, such as Random Forests and Gradient Boosting, which address variance and stability issues more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99340a",
   "metadata": {},
   "source": [
    "## 1. Representation\n",
    "### 1.1 **Domain Set**\n",
    "We define the domain space as  \n",
    "\n",
    "In the CART classification setting, each training example is represented as a feature vector in an $n$-dimensional real space:\n",
    "\n",
    "$$\n",
    "\\mathcal{X} = \\mathbb{R}^n, \\qquad x_i = (x_{i1}, x_{i2}, \\dots, x_{in}) \\in \\mathcal{X}.\n",
    "$$\n",
    "\n",
    "Each component $x_{ij}$ represents the value of feature $j$ for sample $i$.\n",
    "The feature domain can include continuous or categorical variables (encoded numerically in practice).\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 **Label Set**\n",
    "\n",
    "For a $K$-class classification task, the label space is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{Y} = \\{0, 1, \\dots, K-1\\}.\n",
    "$$\n",
    "\n",
    "In the binary case, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathcal{Y} = \\{0, 1\\}.\n",
    "$$\n",
    "\n",
    "### 1.3 **Training Data**\n",
    "\n",
    "We are given a labeled dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N},\n",
    "\\quad x_i \\in \\mathcal{X}, \\; y_i \\in \\mathcal{Y}.\n",
    "$$\n",
    "\n",
    "Each pair $(x_i, y_i)$ represents one training example.\n",
    "The training process recursively partitions $\\mathcal{D}$ based on feature thresholds to form a binary decision tree.\n",
    "\n",
    "### 1.4 **Learner's Output**\n",
    "Formally, the hypothesis space of CART classification is defined as the set of **binary decision trees** of depth at most $T_{\\max}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{H} =\n",
    "\\{\\, h : \\mathcal{X} \\to \\mathcal{Y} \\mid\n",
    "h \\text{ is a binary decision tree with depth } \\le T_{\\max} \\,\\}.\n",
    "$$\n",
    "\n",
    "Each decision tree $h \\in \\mathcal{H}$ recursively partitions the input space $\\mathcal{X}$ into at most $2^{T_{\\max}}$ disjoint leaves.\n",
    "\n",
    "At prediction time, a new observation $x$ is passed through the sequence of feature tests $(x_f \\le t)$ until it reaches a leaf node $i$.\n",
    "Each leaf stores an empirical class probability vector\n",
    "\n",
    "$$\n",
    "p_i = (p_{i,0}, p_{i,1}, \\dots, p_{i,K-1}),\n",
    "$$\n",
    "\n",
    "computed from the training samples that reached that leaf.\n",
    "\n",
    "The predicted class label is then determined by\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\arg\\max_k \\, p_{i,k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb1f67",
   "metadata": {},
   "source": [
    "## 2. Loss\n",
    "In the classification setting, losses are the **measures of impurity**.  CART minimizes impruity and the loss is defined per split. Generally speaking, **Gini** and **Entropy** are good measures.\n",
    "\n",
    "To compute Loss, we need: \n",
    "* Impurity measure, \n",
    "* Split loss based on chosen impurity measure.\n",
    "\n",
    "In the scikit-learn, this is determined by the parameter **criterion**: *{“gini”, “entropy”, “log_loss”}, default=”gini”* \n",
    "\n",
    "\n",
    "### 2.1 **Impurity Function**\n",
    "\n",
    "For a $K$-class classification problem, consider node $i$ containing a subset of samples\n",
    "\n",
    "$$S_i = \\{(x_j, y_j)\\}_{j \\in \\mathcal{I}_i}, \\qquad N_i = |S_i|.$$\n",
    "\n",
    "The number of samples in node $i$ that belong to class $k$ is\n",
    "\n",
    "$$n_{i,k} = \\sum_{j \\in \\mathcal{I}_i} \\mathbf{1}(y_j = k).$$\n",
    "\n",
    "The class proportion of class $k$ in node $i$ is\n",
    "\n",
    "$$p_{i,k} = \\frac{n_{i,k}}{N_i}, \\qquad k = 1, \\dots, K.$$\n",
    "$$\\sum_{k=1}^K p_{i,k} = 1,\\text{and  } p_{i,k} \\ge 0 \\quad \\text{for } k = 1, \\dots, K.$$\n",
    "\n",
    "#### 2.1.1 **Gini**\n",
    "\n",
    "\n",
    "- The Gini impurity of node $i$ is:   \n",
    "$$G_i = 1 - \\sum_{k=1}^K p_{i,k}^2.$$\n",
    "\n",
    "#### 2.1.2 **Entropy**\n",
    "\n",
    "- The entropy impurity of node $i$ is\n",
    "$$H_i = - \\sum_{k=1}^K p_{i,k} \\log p_{i,k},$$\n",
    "\n",
    "- And we assume $0 \\log 0 = 0$.\n",
    "\n",
    "#### 2.2 **Split Loss**\n",
    "\n",
    "Given a candidate split $\\theta$ applied at node $i$, the dataset $S_i$ is partitioned into a left subset $S_i^{\\text{left}}(\\theta)$ and a right subset $S_i^{\\text{right}}(\\theta)$:\n",
    "\n",
    "$$\n",
    "S_i^{\\text{left}}(\\theta) = \\{(x_j, y_j) \\in S_i \\mid x_{j, f} \\le t\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_i^{\\text{right}}(\\theta) = S_i \\setminus S_i^{\\text{left}}(\\theta),\n",
    "$$\n",
    "\n",
    "where $\\theta = (f, t)$ denotes the split feature index $f$ and the threshold value $t$.\n",
    "\n",
    "Let the number of samples in the left and right subsets be\n",
    "\n",
    "$$\n",
    "N_i^{\\text{left}} = |S_i^{\\text{left}}(\\theta)|, \\qquad \n",
    "N_i^{\\text{right}} = |S_i^{\\text{right}}(\\theta)|.\n",
    "$$\n",
    "\n",
    "Their corresponding class proportions are computed in the same way as in Section 2.1.\n",
    "\n",
    "\n",
    "#### 2.2.1 **Weighted Child Impurity**\n",
    "\n",
    "Given an impurity function $C(\\cdot)$ (e.g., Gini or entropy), the **split loss** at node $i$ for candidate split $\\theta$ is defined as the weighted sum of the left and right child impurities:\n",
    "    $$\n",
    "    L(S_i, \\theta) \n",
    "    = \n",
    "    \\frac{N_i^{\\text{left}}}{N_i} \n",
    "    \\, C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)\n",
    "    \\;+\\;\n",
    "    \\frac{N_i^{\\text{right}}}{N_i}\n",
    "    \\, C\\!\\left(S_i^{\\text{right}}(\\theta)\\right).\n",
    "    $$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)$ is the impurity (Gini or entropy) of the left child node.\n",
    "- $C\\!\\left(S_i^{\\text{right}}(\\theta)\\right)$ is the impurity of the right child node.\n",
    "\n",
    "\n",
    "#### 2.2.2 **Optimal Split Selection**\n",
    "\n",
    "The optimal split parameter is chosen by minimizing the split loss:\n",
    "\n",
    "$$\n",
    "\\theta^{*} = \\arg\\min_{\\theta} \\; L(S_i, \\theta).\n",
    "$$\n",
    "\n",
    "And this will be futher explained in the next part, Optimizer on how to actually implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea4014",
   "metadata": {},
   "source": [
    "## 3. Optimizer\n",
    "\n",
    "### 3.1 What is Optimized in CART\n",
    "\n",
    "CART performs a **greedy, recursive partitioning** - at each node, it selects the best split that maximizes information gain (or equivalently minimizes impurity).\n",
    "\n",
    "So the optimizer is essentially a **greedy search algorithm** that finds:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{(f,t)} \\; \\text{Impurity}(S_{\\text{left}}) + \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "where $f$ is the feature and $t$ is the threshold.\n",
    "\n",
    "#### 3.1.1 Objective Function\n",
    "\n",
    "CART minimizes an **impurity measure** (loss function) such as:\n",
    "- Gini Index:\n",
    "$$ G(S) = 1 - \\sum_{k=1}^{K}p_k^2 $$\n",
    "- Entropy:\n",
    "$$ H(S) = - \\sum_{k=1}^{K}p_klog(p_k)$$\n",
    "\n",
    "At each node:\n",
    "$$\n",
    "\\text{Gain}(S, f, t) = \\text{Impurity}(S) \n",
    "- \\frac{|S_{\\text{left}}|}{|S|} \\, \\text{Impurity}(S_{\\text{left}}) \n",
    "- \\frac{|S_{\\text{right}}|}{|S|} \\, \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "The algorithm chooses the feature $f*$ and threshold $t*$ that maximize this gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32546b3d",
   "metadata": {},
   "source": [
    "#### 3.1.2 Pseudo-code\n",
    "\n",
    "Intuitively, the algorithm asks: “Which feature and cutoff most cleanly separates the classes?” By evaluating all possible splits and picking the one that reduces impurity the most, CART greedily chooses the single question that best organizes the data at this point in the tree. This local optimization step is repeated recursively to grow the whole decision tree.\n",
    "\n",
    "In below, we have two parts of pseudo-code, explain how CART 1) build the tree recursively, and 2) optimize for the best split at each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260844c",
   "metadata": {},
   "source": [
    "<div style=\"background:#dbe9ff; padding:10px; border-radius:6px;\">\n",
    "<h3 style=\"margin-top:0;\">Pseudo-Code: Tree Construction (with Explicit Stopping Criteria)</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background:#fff4ce; padding:12px; border-radius:6px; font-size:15px;\">\n",
    "<b>Goal:</b> Recursively grow the CART tree using greedy splits and well-defined stopping rules.\n",
    "</div>\n",
    "\n",
    "<pre style=\"background:#f6f8fa; padding:14px; border:1px solid #ccc; border-radius:6px; font-size:15px;\">\n",
    "\n",
    "FUNCTION <span style=\"color:#0b6fa4;\"><b>BUILD_TREE</b></span>(S_i, depth):\n",
    "\n",
    "    imp ← impurity(S_i)\n",
    "    n_samples ← number of samples in S_i\n",
    "\n",
    "    1. IF <span style=\"color:#d14;\"><b>imp == 0</b></span>:\n",
    "           # Node is pure — all samples belong to the same class\n",
    "           RETURN LeafNode(class_distribution(S_i))\n",
    "\n",
    "    2. IF <span style=\"color:#d14;\"><b>n_samples < min_samples_split</b></span>:\n",
    "           # Too few samples to reliably split\n",
    "           RETURN LeafNode(class_distribution(S_i))\n",
    "\n",
    "    3. IF <span style=\"color:#d14;\"><b>depth ≥ max_depth</b></span>:\n",
    "           # Depth limit reached\n",
    "           RETURN LeafNode(class_distribution(S_i))\n",
    "\n",
    "    4. IF <span style=\"color:#d14;\"><b>no_valid_threshold_exists(S_i)</b></span>:\n",
    "           # All features have only one unique value, or all splits produce empty child nodes\n",
    "           RETURN LeafNode(class_distribution(S_i))\n",
    " \n",
    "    (j*, t*) ← FIND_BEST_SPLIT(S_i)\n",
    "\n",
    "    IF j* is None:\n",
    "        RETURN LeafNode(class_distribution(S_i))\n",
    "\n",
    "    S_left  ← samples in S_i where x_j* ≤ t*\n",
    "    S_right ← samples in S_i where x_j* >  t*\n",
    "\n",
    "    LeftChild  ← BUILD_TREE(S_left,  depth + 1)\n",
    "    RightChild ← BUILD_TREE(S_right, depth + 1)\n",
    "\n",
    "    RETURN InternalNode(\n",
    "                feature = j*,\n",
    "                threshold = t*,\n",
    "                left = LeftChild,\n",
    "                right = RightChild\n",
    "           )\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f294058",
   "metadata": {},
   "source": [
    "<div style=\"background:#dbe9ff; padding:10px; border-radius:6px;\">\n",
    "<h3 style=\"margin-top:0;\">Pseudo-Code: Best Split Search</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background:#fff4ce; padding:12px; border-radius:6px; font-size:15px;\">\n",
    "<b>Goal:</b> find the best feature-threshold pair <code>(j*, t*)</code> minimizing node loss.\n",
    "</div>\n",
    "\n",
    "<pre style=\"background:#f6f8fa; padding:14px; border:1px solid #ccc; border-radius:6px; font-size:15px;\">\n",
    "FUNCTION <span style=\"color:#0b6fa4;\"><b>FIND_BEST_SPLIT</b></span>(S_i):\n",
    "\n",
    "    best_feature ← None\n",
    "    best_threshold ← None\n",
    "    best_loss ← +∞\n",
    "\n",
    "    FOR each feature <span style=\"color:#d14;\">j</span> in 1..d:\n",
    "\n",
    "        x_j ← column j of S_i\n",
    "        v ← sorted unique values of x_j\n",
    "\n",
    "        IF length(v) == 1:\n",
    "            CONTINUE  // no valid split\n",
    "\n",
    "        T_j ← midpoints of consecutive values in v\n",
    "\n",
    "        FOR each t in T_j:\n",
    "\n",
    "            S_left  ← samples with x_j ≤ t\n",
    "            S_right ← samples with x_j > t\n",
    "\n",
    "            IF S_left empty OR S_right empty:\n",
    "                CONTINUE\n",
    "\n",
    "            imp_left  ← impurity(S_left)\n",
    "            imp_right ← impurity(S_right)\n",
    "\n",
    "            loss ← (|S_left|/|S_i|)*imp_left + (|S_right|/|S_i|)*imp_right\n",
    "\n",
    "            IF loss < best_loss:\n",
    "                best_loss ← loss\n",
    "                best_feature ← j\n",
    "                best_threshold ← t\n",
    "\n",
    "    RETURN (best_feature, best_threshold)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081dce5",
   "metadata": {},
   "source": [
    "<div style=\"background:#dbe9ff; padding:10px; border-radius:6px;\">\n",
    "<h3 style=\"margin:0;\">Pseudo-Code: <span style=\"color:#0b6fa4;\">Cost-Complexity Pruning Path</span></h3>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"background:#fff4ce; padding:12px; border-radius:6px; font-size:15px;\">\n",
    "<b>Goal:</b> find the pruning path.\n",
    "</div>\n",
    "\n",
    "\n",
    "<pre style=\"background:#f6f8fa; padding:14px; border:1px solid #ccc; border-radius:6px; font-size:15px;\">\n",
    "\n",
    "FUNCTION <span style=\"color:#0b6fa4;\"><b>COST_COMPLEXITY_PRUNING_PATH</b></span>(tree):\n",
    "\n",
    "    <span style=\"color:#b54700;\"># Step 1 — Compute subtree impurity and leaf count (bottom-up)</span>\n",
    "    FOR each node t (post-order traversal):\n",
    "        IF t is a leaf:\n",
    "            R_subtree[t] ← impurity(t) * sample_count(t)\n",
    "            num_leaves[t] ← 1\n",
    "        ELSE:\n",
    "            R_subtree[t] ← R_subtree[left_child(t)] + R_subtree[right_child(t)]\n",
    "            num_leaves[t] ← num_leaves[left_child(t)] + num_leaves[right_child(t)]\n",
    "\n",
    "    <span style=\"color:#b54700;\"># Step 2 — Compute the \"cost of pruning\" αₜ for each internal node</span>\n",
    "    FOR each internal node t:\n",
    "        R_leaf ← impurity( t treated as a leaf ) * sample_count(t)\n",
    "        α_t ← ( R_leaf - R_subtree[t] ) / ( num_leaves[t] - 1 )\n",
    "\n",
    "    <span style=\"color:#b54700;\"># Step 3 — Iteratively prune subtrees with the smallest αₜ</span>\n",
    "    α_list ← [0]\n",
    "    impurity_list ← [ R_subtree[root] ]\n",
    "\n",
    "    WHILE the tree can still be pruned:\n",
    "        α_min ← minimum α_t over all remaining internal nodes\n",
    "        prune all subtrees whose α_t = α_min (replace with a leaf)\n",
    "        update the total tree impurity\n",
    "        append α_min to α_list\n",
    "        append updated impurity to impurity_list\n",
    "\n",
    "    RETURN (α_list, impurity_list)\n",
    "\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff4cbc",
   "metadata": {},
   "source": [
    "# Part 2: Model\n",
    "\n",
    "In below is our code for model class, tree class, impurity functions, and other helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8de31-4d21-49ed-a953-18561f05383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def node_score_gini_from_counts(counts):\n",
    "    '''\n",
    "    Compute Gini impurity from class counts directly.\n",
    "    '''\n",
    "    n = int(counts.sum())\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    sum_sq = float((counts ** 2).sum())\n",
    "    return 1.0 - sum_sq / (n * n)\n",
    "\n",
    "def node_score_entropy_from_counts(counts):\n",
    "    '''\n",
    "    Compute Entropy impurity from class counts directly:\n",
    "        H = log2(n) - (1/n) * sum_k c_k * log2(c_k)\n",
    "    '''\n",
    "    n = int(counts.sum())\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    mask = counts > 0\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "\n",
    "    c = counts[mask]\n",
    "    return float(np.log2(n) - (c * np.log2(c)).sum() / n)\n",
    "\n",
    "## Tree Structure to mimic sklearn's DecisionTreeClassifier tree storage\n",
    "class _Tree:\n",
    "    \"\"\"\n",
    "    Simple tree structure storing node info in parallel lists, then\n",
    "    converted to numpy arrays via finalize().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.children_left = []\n",
    "        self.children_right = []\n",
    "        self.feature = []\n",
    "        self.threshold = []\n",
    "        self.impurity = []\n",
    "        self.n_node_samples = []\n",
    "        self.value = []   # class counts per node (1D arrays length n_classes)\n",
    "\n",
    "    def add_node(self, feature, threshold, impurity,\n",
    "                 n_node_samples, counts, left=-1, right=-1):\n",
    "        \"\"\"\n",
    "        Append a node and return its node_id (index).\n",
    "        feature = -1 means leaf.\n",
    "        \"\"\"\n",
    "        node_id = len(self.feature)\n",
    "        self.children_left.append(int(left))\n",
    "        self.children_right.append(int(right))\n",
    "        self.feature.append(int(feature))\n",
    "        self.threshold.append(float(threshold))\n",
    "        self.impurity.append(float(impurity))\n",
    "        self.n_node_samples.append(int(n_node_samples))\n",
    "        self.value.append(np.asarray(counts, dtype=np.int64))\n",
    "        return node_id\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"\n",
    "        Convert internal Python lists to numpy arrays.\n",
    "        \"\"\"\n",
    "        self.children_left = np.asarray(self.children_left, dtype=np.int32)\n",
    "        self.children_right = np.asarray(self.children_right, dtype=np.int32)\n",
    "        self.feature = np.asarray(self.feature, dtype=np.int32)\n",
    "        self.threshold = np.asarray(self.threshold, dtype=np.float64)\n",
    "        self.impurity = np.asarray(self.impurity, dtype=np.float64)\n",
    "        self.n_node_samples = np.asarray(self.n_node_samples, dtype=np.int64)\n",
    "        self.value = np.stack(self.value, axis=0)  # shape (n_nodes, n_classes)\n",
    "        self.node_count = self.feature.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CARTClassifier:\n",
    "    \"\"\"\n",
    "    Numpy-only CART decision tree classifier with:\n",
    "    - criterion: \"gini\" or \"entropy\"\n",
    "    - max_depth: maximum depth of the tree (or None)\n",
    "    - min_sample_split: minimum samples required to split\n",
    "    - alpha: cost-complexity pruning parameter (0 = no pruning)\n",
    "    - random_state: used ONLY to randomly permute feature order at each split\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 criterion=\"gini\",\n",
    "                 max_depth=None,\n",
    "                 min_sample_split=2,\n",
    "                 alpha=0.0,\n",
    "                 random_state=None):\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.alpha = float(alpha)\n",
    "        self.random_state = random_state\n",
    "        self._rng = np.random.RandomState(random_state)\n",
    "        if criterion == \"gini\":\n",
    "            self._impurity_from_counts = node_score_gini_from_counts\n",
    "        else:\n",
    "            self._impurity_from_counts = node_score_entropy_from_counts\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the full tree, then apply cost-complexity pruning with alpha.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.int64)\n",
    "\n",
    "        self.n_samples_, self.n_features_in_ = X.shape\n",
    "\n",
    "        # Handle class labels (0..K-1 or need remap)\n",
    "        classes = np.unique(y)\n",
    "        if not np.array_equal(classes, np.arange(classes.size)):\n",
    "            # remap to 0..K-1\n",
    "            self._class_mapping_ = {c: i for i, c in enumerate(classes)}\n",
    "            y_enc = np.array([self._class_mapping_[c] for c in y], dtype=np.int64)\n",
    "            self.classes_ = classes\n",
    "        else:\n",
    "            self._class_mapping_ = None\n",
    "            y_enc = y\n",
    "            self.classes_ = classes\n",
    "\n",
    "        self.n_classes_ = self.classes_.size\n",
    "\n",
    "        # Build full (unpruned) tree\n",
    "        self.tree_ = _Tree(n_classes=self.n_classes_)\n",
    "        indices = np.arange(self.n_samples_, dtype=np.int64)\n",
    "        self._build_tree(X, y_enc, indices, depth=0)\n",
    "        self.tree_.finalize()\n",
    "\n",
    "        # Cost-complexity pruning with given alpha\n",
    "        if self.alpha > 0.0:\n",
    "            self._prune_tree()\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Internal helpers\n",
    "    def _class_counts(self, y_subset):\n",
    "        return np.bincount(y_subset, minlength=self.n_classes_)\n",
    "        \n",
    "    def _build_tree(self, X, y, indices, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the tree using greedy splitting.\n",
    "        Returns node_id of the root of this subtree.\n",
    "        \"\"\"\n",
    "        y_node = y[indices]\n",
    "        counts = self._class_counts(y_node)\n",
    "        n_node_samples = indices.size\n",
    "        impurity = self._impurity_from_counts(counts)\n",
    "\n",
    "        # Stopping criteria\n",
    "        # 1) Pure node (allow tiny negative -0.0 from fp)\n",
    "        if impurity <= 0.0:\n",
    "            return self.tree_.add_node(feature=-1,threshold=-1.0,impurity=impurity,n_node_samples=n_node_samples,counts=counts,left=-1,right=-1)\n",
    "\n",
    "        # 2) Too few samples\n",
    "        if n_node_samples < self.min_sample_split:\n",
    "            return self.tree_.add_node(feature=-1,threshold=-1.0,impurity=impurity,n_node_samples=n_node_samples,counts=counts,left=-1,right=-1)\n",
    "\n",
    "        # 3) Depth limit\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return self.tree_.add_node(feature=-1,threshold=-1.0,impurity=impurity,n_node_samples=n_node_samples,counts=counts,left=-1,right=-1)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold, best_loss = self._find_best_split(X, y_node, indices)\n",
    "\n",
    "        # 4) No valid split found -> leaf\n",
    "        if best_feature is None:\n",
    "            return self.tree_.add_node(feature=-1,threshold=-1.0,impurity=impurity,n_node_samples=n_node_samples,counts=counts,left=-1,right=-1)\n",
    "\n",
    "        # Partition samples\n",
    "        x_best = X[indices, best_feature]\n",
    "        left_mask = x_best <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Safety: if split degenerate, fallback to leaf\n",
    "        if (not np.any(left_mask)) or (not np.any(right_mask)):\n",
    "            return self.tree_.add_node(feature=-1,threshold=-1.0,impurity=impurity,n_node_samples=n_node_samples,counts=counts,left=-1,right=-1)\n",
    "\n",
    "        idx_left = indices[left_mask]\n",
    "        idx_right = indices[right_mask]\n",
    "\n",
    "        # Create internal node (children set after recursion)\n",
    "        node_id = self.tree_.add_node(feature=int(best_feature),threshold=float(best_threshold),\n",
    "                                      impurity=float(impurity),n_node_samples=int(n_node_samples),counts=counts,left=-1,right=-1)\n",
    "\n",
    "        # Recursively build children\n",
    "        left_child = self._build_tree(X, y, idx_left, depth + 1)\n",
    "        right_child = self._build_tree(X, y, idx_right, depth + 1)\n",
    "\n",
    "        # Patch children pointers\n",
    "        self.tree_.children_left[node_id] = left_child\n",
    "        self.tree_.children_right[node_id] = right_child\n",
    "\n",
    "        return node_id\n",
    "\n",
    "    def _find_best_split(self, X, y_node, indices):\n",
    "        \"\"\"\n",
    "        Find best (feature, threshold) for the node defined by `indices`.\n",
    "        \"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_loss = np.inf\n",
    "        EPS = 1e-12  \n",
    "\n",
    "        # per-node random permutation of features\n",
    "        feature_indices = self._rng.permutation(self.n_features_in_)\n",
    "\n",
    "        y_sub = y_node  \n",
    "\n",
    "        for j in feature_indices:\n",
    "            x_j = X[indices, j]\n",
    "            # Sort by feature value\n",
    "            order = np.argsort(x_j, kind=\"mergesort\")\n",
    "            x_sorted = x_j[order]\n",
    "            y_sorted = y_sub[order]\n",
    "\n",
    "            # No split if all values equal\n",
    "            if x_sorted[0] == x_sorted[-1]:\n",
    "                continue\n",
    "\n",
    "            # Candidate split positions: k where x[k] != x[k+1]\n",
    "            diff = x_sorted[1:] != x_sorted[:-1]\n",
    "            if not np.any(diff):\n",
    "                continue\n",
    "            split_pos = np.nonzero(diff)[0]  # array of k indices\n",
    "\n",
    "            # Initialize class counts\n",
    "            right_counts = self._class_counts(y_sorted)  # all samples start on right\n",
    "            left_counts = np.zeros(self.n_classes_, dtype=np.int64)\n",
    "\n",
    "            # We'll sweep once from left->right, and only evaluate at split_pos.\n",
    "            sp_i = 0\n",
    "            next_k = split_pos[sp_i]\n",
    "\n",
    "            # Move sample k from right to left each step\n",
    "            for k in range(x_sorted.size - 1):\n",
    "                cls = y_sorted[k]\n",
    "                left_counts[cls] += 1\n",
    "                right_counts[cls] -= 1\n",
    "\n",
    "                if k != next_k:\n",
    "                    continue\n",
    "\n",
    "                nL = k + 1\n",
    "                nR = x_sorted.size - nL\n",
    "                if nL == 0 or nR == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    imp_left = self._impurity_from_counts(left_counts)\n",
    "                    imp_right = self._impurity_from_counts(right_counts)\n",
    "                    loss = nL * imp_left + nR * imp_right\n",
    "\n",
    "                    # STRICT improvement only => first best encountered wins\n",
    "                    if loss < best_loss - EPS:\n",
    "                        best_loss = loss\n",
    "                        best_feature = j\n",
    "                        best_threshold = 0.5 * (x_sorted[k] + x_sorted[k + 1])\n",
    "                sp_i += 1\n",
    "                if sp_i >= split_pos.size:\n",
    "                    break\n",
    "                next_k = split_pos[sp_i]\n",
    "        return best_feature, best_threshold, best_loss\n",
    "\n",
    "    # Cost-complexity pruning\n",
    "    def _compute_subtree_stats(self, node_id):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "            R_subtree = sum impurity(leaf) * n_samples(leaf)\n",
    "            n_leaves  = number of leaves\n",
    "        for the subtree rooted at node_id.\n",
    "        \"\"\"\n",
    "        left = self.tree_.children_left[node_id]\n",
    "        right = self.tree_.children_right[node_id]\n",
    "\n",
    "        if left == -1 and right == -1:\n",
    "            R = self.tree_.impurity[node_id] * self.tree_.n_node_samples[node_id]\n",
    "            return R, 1\n",
    "\n",
    "        R_l, L_l = self._compute_subtree_stats(left)\n",
    "        R_r, L_r = self._compute_subtree_stats(right)\n",
    "        return R_l + R_r, L_l + L_r\n",
    "\n",
    "    def _prune_tree(self):\n",
    "        \"\"\"\n",
    "        Apply cost-complexity pruning with the given alpha.\n",
    "        Greedy weakest-link strategy:\n",
    "            repeatedly prune the node t with smallest g(t)\n",
    "            as long as g(t) <= alpha.\n",
    "        \"\"\"\n",
    "        alpha = self.alpha\n",
    "        if alpha <= 0.0:\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            n_nodes = self.tree_.node_count\n",
    "            R_subtree = np.zeros(n_nodes, dtype=np.float64)\n",
    "            n_leaves = np.zeros(n_nodes, dtype=np.int64)\n",
    "\n",
    "            def dfs(node_id):\n",
    "                left = self.tree_.children_left[node_id]\n",
    "                right = self.tree_.children_right[node_id]\n",
    "                if left == -1 and right == -1:\n",
    "                    R = self.tree_.impurity[node_id] * self.tree_.n_node_samples[node_id]\n",
    "                    R_subtree[node_id] = R\n",
    "                    n_leaves[node_id] = 1\n",
    "                    return R, 1\n",
    "                R_l, L_l = dfs(left)\n",
    "                R_r, L_r = dfs(right)\n",
    "                R_subtree[node_id] = R_l + R_r\n",
    "                n_leaves[node_id] = L_l + L_r\n",
    "                return R_l + R_r, L_l + L_r\n",
    "\n",
    "            dfs(0)\n",
    "\n",
    "            g = np.full(n_nodes, np.inf, dtype=np.float64)\n",
    "            for node_id in range(n_nodes):\n",
    "                left = self.tree_.children_left[node_id]\n",
    "                right = self.tree_.children_right[node_id]\n",
    "                if left == -1 and right == -1:\n",
    "                    continue  # leaf\n",
    "                if n_leaves[node_id] <= 1:\n",
    "                    continue\n",
    "\n",
    "                R_leaf = self.tree_.impurity[node_id] * self.tree_.n_node_samples[node_id]\n",
    "                R_T = R_subtree[node_id]\n",
    "                denom = n_leaves[node_id] - 1\n",
    "                if denom <= 0:\n",
    "                    continue\n",
    "\n",
    "                g[node_id] = (R_leaf - R_T) / denom\n",
    "\n",
    "            min_g = g.min()\n",
    "            if (not np.isfinite(min_g)) or (min_g > alpha):\n",
    "                break\n",
    "\n",
    "            node_to_prune = int(np.argmin(g))\n",
    "            self.tree_.children_left[node_to_prune] = -1\n",
    "            self.tree_.children_right[node_to_prune] = -1\n",
    "\n",
    "    # Prediction\n",
    "    def _predict_one_proba(self, x):\n",
    "        \"\"\"\n",
    "        Traverse the tree for a single sample x and return class probabilities.\n",
    "        \"\"\"\n",
    "        node = 0\n",
    "        while True:\n",
    "            feature = self.tree_.feature[node]\n",
    "            if feature == -1:\n",
    "                counts = self.tree_.value[node]\n",
    "                total = counts.sum()\n",
    "                if total == 0:\n",
    "                    return np.ones(self.n_classes_) / self.n_classes_\n",
    "                return counts / total\n",
    "\n",
    "            thr = self.tree_.threshold[node]\n",
    "            if x[feature] <= thr:\n",
    "                node = self.tree_.children_left[node]\n",
    "            else:\n",
    "                node = self.tree_.children_right[node]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        proba = np.zeros((n_samples, self.n_classes_), dtype=np.float64)\n",
    "        for i in range(n_samples):\n",
    "            proba[i] = self._predict_one_proba(X[i])\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        class_indices = np.argmax(proba, axis=1)\n",
    "        return self.classes_[class_indices]\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute misclassification loss on (X, y).\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred != y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute accuracy on (X, y).\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1094318",
   "metadata": {},
   "source": [
    "# Part 3: Check Model\n",
    "\n",
    "In this section, we design unit tests for our `DecisionTreeCART` implementation and compare it against `sklearn.tree.DecisionTreeClassifier` on a public dataset (the breast cancer dataset). The goals are:\n",
    "- verify that each method of our class works correctly in isolation,\n",
    "- check that edge cases are handled properly,\n",
    "- and demonstrate that our implementation can successfully reproduce sklearn’s CART results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "930ad69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "import random\n",
    "\n",
    "# 1. our CART wrapper\n",
    "def make_our_cart(X_train,X_test,y_train,y_test,*, criterion=\"gini\",max_depth=None,min_sample_split=2,alpha=0.0,random_state=None,verbose=True):\n",
    "    \"\"\"\n",
    "    Train our numpy CART on given train/test split and print acc/loss.\n",
    "    loss = 1 - accuracy (0-1 loss)\n",
    "    \"\"\"\n",
    "    clf = CARTClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_sample_split=min_sample_split,\n",
    "        alpha=alpha,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = clf.accuracy(X_test, y_test)\n",
    "    loss = clf.loss(X_test, y_test)\n",
    "    if verbose:\n",
    "        print(f\"[OUR CART] acc={acc:.4f}, loss={loss:.4f}\")\n",
    "    return clf, acc, loss, y_pred\n",
    "\n",
    "\n",
    "# 2. sklearn CART wrapper\n",
    "def make_sk_cart(X_train,X_test,y_train,y_test,*,criterion=\"gini\",max_depth=None,min_sample_split=2,random_state=None,verbose=True):\n",
    "    \"\"\"\n",
    "    Train sklearn's DecisionTreeClassifier and print acc/loss.\n",
    "    \"\"\"\n",
    "    sk_clf = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_sample_split,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    sk_clf.fit(X_train, y_train)\n",
    "    y_pred = sk_clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    loss = 1.0 - acc\n",
    "    if verbose:\n",
    "        print(f\"[SK CART ] acc={acc:.4f}, loss={loss:.4f}\")\n",
    "    return sk_clf, acc, loss, y_pred\n",
    "\n",
    "# 3. test_on_dataset(): loop over seeds, compare our vs sklearn\n",
    "def test_on_dataset(X,y,seed_list,*,criterion=\"gini\",max_depth=None,min_sample_split=2,alpha=0.0,test_size=0.3):\n",
    "    \"\"\"\n",
    "    For each random seed:\n",
    "      - create the same train/test split\n",
    "      - train our CART and sklearn CART\n",
    "      - compare accuracy and loss\n",
    "      - print mismatched predictions (our vs sklearn) on X_test\n",
    "    \"\"\"\n",
    "    print(\"==============================================\")\n",
    "    print(\"Testing on dataset with seeds:\", seed_list)\n",
    "    print(\"criterion =\", criterion,\n",
    "          \"max_depth =\", max_depth,\n",
    "          \"min_sample_split =\", min_sample_split,\n",
    "          \"alpha =\", alpha)\n",
    "    print(\"==============================================\\n\")\n",
    "\n",
    "    for seed in seed_list:\n",
    "        print(f\"--- Seed = {seed} ---\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=seed,stratify=y)\n",
    "\n",
    "        # our CART\n",
    "        our_clf, our_acc, our_loss, y_pred_our = make_our_cart(X_train,X_test,y_train,y_test,criterion=criterion,\n",
    "                                                               max_depth=max_depth,min_sample_split=min_sample_split,alpha=alpha,random_state=seed,verbose=True)\n",
    "\n",
    "        # sklearn CART\n",
    "        sk_clf, sk_acc, sk_loss, y_pred_sk = make_sk_cart(X_train,X_test,y_train,y_test,criterion=criterion,\n",
    "                                                          max_depth=max_depth,min_sample_split=min_sample_split,random_state=seed,verbose=True)\n",
    "\n",
    "        # compare\n",
    "        acc_diff = our_acc - sk_acc\n",
    "        loss_diff = our_loss - sk_loss\n",
    "        print(f\"Diff: acc (our - sk) = {acc_diff:+.4f}, \"\n",
    "              f\"loss (our - sk) = {loss_diff:+.4f}\")\n",
    "\n",
    "        mismatch_mask = (y_pred_our != y_pred_sk)\n",
    "        mismatch_idx = np.where(mismatch_mask)[0]\n",
    "\n",
    "        if mismatch_idx.size == 0:\n",
    "            print(\"  [MATCH] our predictions == sklearn predictions on all test samples.\\n\")\n",
    "        else:\n",
    "            print(f\"  [MISMATCH] {mismatch_idx.size} samples have different predictions:\")\n",
    "            for i in mismatch_idx:\n",
    "                print(f\"    test_idx={i}: \"\n",
    "                      f\"y_true={y_test[i]}, \"\n",
    "                      f\"y_our={y_pred_our[i]}, \"\n",
    "                      f\"y_sk={y_pred_sk[i]}\")\n",
    "            print()\n",
    "\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad69b75",
   "metadata": {},
   "source": [
    "### Test 1–3: Basic functionality\n",
    "\n",
    "- **Test 1 – fit():** check that training runs without error.  \n",
    "- **Test 2 – predict():** check output shape, label range, and report train/test accuracy.  \n",
    "- **Test 3 – loss():** check that `loss` returns a finite scalar (misclassification error in [0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54ce17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: train() runs without error.\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "clf = CARTClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Test 1 passed: train() runs without error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b8d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 passed: predict() shape & value checks passed. Train accuracy = 1.000\n",
      "Test accuracy = 0.900\n"
     ]
    }
   ],
   "source": [
    "# Test 2: predict() should produce outputs with correct shape and valid class values\n",
    "clf = CARTClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "\n",
    "# Check output shape\n",
    "assert y_pred_train.shape == y_train.shape, f\"Prediction shape mismatch: y_pred shape={y_pred_train.shape}, y_train shape={y_train.shape}\"\n",
    "\n",
    "# Check value range (breast_cancer is a binary classification dataset)\n",
    "unique_vals = np.unique(y_pred_train)\n",
    "assert set(unique_vals).issubset({0, 1}),  f\"Predicted values must be 0/1. Found values: {unique_vals}\"\n",
    "\n",
    "train_acc = clf.accuracy(X_train, y_train)\n",
    "print(f\"Test 2 passed: predict() shape & value checks passed. Train accuracy = {train_acc:.3f}\")\n",
    "\n",
    "test_acc = clf.accuracy(X_test, y_test)\n",
    "print(f\"Test accuracy = {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6a06194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 passed: loss() returns a valid finite scalar. Train loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Test 3: loss() should return a finite scalar value\n",
    "\n",
    "clf = CARTClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_loss = clf.loss(X_train, y_train)\n",
    "\n",
    "assert np.isscalar(train_loss), \"loss() should return a scalar value.\"\n",
    "assert np.isfinite(train_loss), \"loss() should not return NaN or infinity.\"\n",
    "\n",
    "print(f\"Test 3 passed: loss() returns a valid finite scalar. Train loss = {train_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747a13e",
   "metadata": {},
   "source": [
    "Our loss function is defined as the misclassification error rate, therefore it should be a scalar between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64412045",
   "metadata": {},
   "source": [
    "### Test 4: Edge cases and method-level tests\n",
    "\n",
    "We use small toy datasets to verify that our CART implementation behaves correctly under extreme scenarios and that core methods work as intended.\n",
    "\n",
    "**Edge cases**\n",
    "- **Test 4.1 – All labels identical (only one class)**\n",
    "- **Test 4.2 – Single feature only**\n",
    "- **Test 4.3 – All-zero features**\n",
    "\n",
    "**Method-level tests**\n",
    "- **Test 4.4 – `predict_proba()`**: correct shape, valid probability distribution, and consistency with `predict`\n",
    "- **Test 4.5 – `accuracy()`**: matches manual computation on a tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5eb1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_toy:\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "y_toy: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# A small toy dataset for edge case testing\n",
    "X_toy = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "])\n",
    "y_toy = np.array([0, 0, 1, 1])\n",
    "print(\"X_toy:\\n\", X_toy)\n",
    "print(\"y_toy:\", y_toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45cb7275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.1 passed: all-zero labels edge case handled correctly.\n",
      "Predicted labels: [0 0 0 0]\n",
      "Loss on all-zero labels: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test 4.1: all labels are zero (only one class present)\n",
    "clf_zero = CARTClassifier()\n",
    "\n",
    "y_all_zero = np.zeros_like(y_toy)\n",
    "clf_zero.fit(X_toy, y_all_zero)\n",
    "\n",
    "y_pred_zero = clf_zero.predict(X_toy)\n",
    "loss_zero = clf_zero.loss(X_toy, y_all_zero)\n",
    "\n",
    "assert y_pred_zero.shape == y_all_zero.shape\n",
    "assert np.isfinite(loss_zero)\n",
    "\n",
    "print(\"Test 4.1 passed: all-zero labels edge case handled correctly.\")\n",
    "print(\"Predicted labels:\", y_pred_zero)\n",
    "print(\"Loss on all-zero labels:\", loss_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdeffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.2 passed: single-feature edge case handled correctly.\n",
      "Predicted labels: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test 4.2: dataset contains only one feature\n",
    "\n",
    "model_single = CARTClassifier()\n",
    "\n",
    "X_single = X_toy[:, :1]  # Use only the first feature\n",
    "model_single.fit(X_single, y_toy)\n",
    "\n",
    "y_pred_single = model_single.predict(X_single)\n",
    "assert y_pred_single.shape == y_toy.shape\n",
    "\n",
    "print(\"Test 4.2 passed: single-feature edge case handled correctly.\")\n",
    "\n",
    "assert np.array_equal(y_pred_single, y_toy)\n",
    "print(\"Predicted labels:\", y_pred_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f302c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.3 passed: all-zero features edge case handled correctly.\n",
      "Predicted labels: [0 0 0 0]\n",
      "Loss on all-zero features: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Test 4.3: all feature values are zero\n",
    "\n",
    "model_feat_zero = CARTClassifier()\n",
    "\n",
    "X_zeros = np.zeros_like(X_toy)\n",
    "model_feat_zero.fit(X_zeros, y_toy)\n",
    "\n",
    "y_pred_zeros = model_feat_zero.predict(X_zeros)\n",
    "loss_zeros = model_feat_zero.loss(X_zeros, y_toy)\n",
    "\n",
    "assert y_pred_zeros.shape == y_toy.shape\n",
    "assert np.isfinite(loss_zeros)\n",
    "\n",
    "print(\"Test 4.3 passed: all-zero features edge case handled correctly.\")\n",
    "print(\"Predicted labels:\", y_pred_zeros)\n",
    "print(\"Loss on all-zero features:\", loss_zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e6fd4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4b passed: predict_proba has correct shape, rows sum to 1, and argmax matches predict().\n"
     ]
    }
   ],
   "source": [
    "# Test 4.4: predict_proba() shape and probabilities\n",
    "\n",
    "model = CARTClassifier(max_depth=5, min_sample_split=2, criterion='gini')\n",
    "model.fit(X_train, y_train)\n",
    "proba = model.predict_proba(X_test)\n",
    "n_classes = len(np.unique(y_train))\n",
    "assert proba.shape == (X_test.shape[0], n_classes),f\"predict_proba shape {proba.shape} does not match (n_samples, n_classes).\"\n",
    "\n",
    "row_sums = proba.sum(axis=1)\n",
    "assert np.allclose(row_sums, 1.0, atol=1e-12), \"Each row of predict_proba should sum to 1.\"\n",
    "\n",
    "y_pred_from_proba = np.argmax(proba, axis=1)\n",
    "y_pred = model.predict(X_test)\n",
    "assert np.array_equal(y_pred_from_proba, y_pred), \"argmax over predict_proba should match predict().\"\n",
    "print(\"Test 4b passed: predict_proba has correct shape, rows sum to 1, and argmax matches predict().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4ecf3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4c passed: model.accuracy matches manual accuracy on the toy dataset.\n"
     ]
    }
   ],
   "source": [
    "# Test 4.5: accuracy() matches manual computation\n",
    "X_toy_small = np.array([[0], [1], [2], [3]])\n",
    "y_toy_small = np.array([0, 0, 1, 1])\n",
    "\n",
    "model = CARTClassifier(max_depth=2, min_sample_split=2, criterion='gini')\n",
    "model.fit(X_toy_small, y_toy_small)\n",
    "\n",
    "y_pred_toy = model.predict(X_toy_small)\n",
    "manual_acc = np.mean(y_pred_toy == y_toy_small)\n",
    "model_acc = model.accuracy(X_toy_small, y_toy_small)\n",
    "\n",
    "assert np.isclose(manual_acc, model_acc), f\"Manual accuracy {manual_acc} does not match model.accuracy {model_acc}.\"\n",
    "\n",
    "print(\"Test 4c passed: model.accuracy matches manual accuracy on the toy dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da70f4",
   "metadata": {},
   "source": [
    "### Test 5: Comparison with sklearn on a public dataset\n",
    "\n",
    "We now compare our `CARTClassifier` implementation to `sklearn.tree.DecisionTreeClassifier` on the iris dataset.  \n",
    "\n",
    "\n",
    "We perform two comparisons:\n",
    "- **Test 5.1 – Gini impurity**\n",
    "- **Test 5.2 – Entropy impurity**\n",
    "- **Test 5.3 - Gini Impurity with Pruning**\n",
    "- **Test 5.4 - Entropy impurity with Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e31cf2",
   "metadata": {},
   "source": [
    "#### Test 5.1 Gini Impurity \n",
    "\n",
    "- Test on multiple seeds\n",
    "- `max_depth=5`\n",
    "- `criterion='gini'`\n",
    "- `alpha=0`: No pruning\n",
    "\n",
    "We reached exact same accuracy as Sklearn results on these seeds, and on dataset iris and breast_cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "337776e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [0, 1, 3, 5, 10, 99]\n",
      "criterion = gini max_depth = 5 min_sample_split = 2 alpha = 0.0\n",
      "==============================================\n",
      "\n",
      "--- Seed = 0 ---\n",
      "[OUR CART] acc=0.9778, loss=0.0222\n",
      "[SK CART ] acc=0.9778, loss=0.0222\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 1 ---\n",
      "[OUR CART] acc=0.9778, loss=0.0222\n",
      "[SK CART ] acc=0.9778, loss=0.0222\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 3 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 5 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 10 ---\n",
      "[OUR CART] acc=1.0000, loss=0.0000\n",
      "[SK CART ] acc=1.0000, loss=0.0000\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 99 ---\n",
      "[OUR CART] acc=0.9778, loss=0.0222\n",
      "[SK CART ] acc=0.9778, loss=0.0222\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "seeds = [0, 1, 3, 5, 10, 99]\n",
    "test_on_dataset(X_iris,y_iris,seed_list=seeds,criterion=\"gini\",max_depth=5,min_sample_split=2,alpha=0.0,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "120fb39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [1, 10, 55, 66, 99]\n",
      "criterion = gini max_depth = 5 min_sample_split = 2 alpha = 0.0\n",
      "==============================================\n",
      "\n",
      "--- Seed = 1 ---\n",
      "[OUR CART] acc=0.9415, loss=0.0585\n",
      "[SK CART ] acc=0.9415, loss=0.0585\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MISMATCH] 2 samples have different predictions:\n",
      "    test_idx=126: y_true=1, y_our=1, y_sk=0\n",
      "    test_idx=165: y_true=0, y_our=1, y_sk=0\n",
      "\n",
      "--- Seed = 10 ---\n",
      "[OUR CART] acc=0.9532, loss=0.0468\n",
      "[SK CART ] acc=0.9532, loss=0.0468\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 55 ---\n",
      "[OUR CART] acc=0.9298, loss=0.0702\n",
      "[SK CART ] acc=0.9298, loss=0.0702\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 66 ---\n",
      "[OUR CART] acc=0.8889, loss=0.1111\n",
      "[SK CART ] acc=0.8889, loss=0.1111\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 99 ---\n",
      "[OUR CART] acc=0.9240, loss=0.0760\n",
      "[SK CART ] acc=0.9240, loss=0.0760\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MISMATCH] 4 samples have different predictions:\n",
      "    test_idx=2: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=19: y_true=0, y_our=0, y_sk=1\n",
      "    test_idx=21: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=118: y_true=0, y_our=0, y_sk=1\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_breast_cancer, y_breast_cancer = load_breast_cancer(return_X_y=True)\n",
    "seeds = [1,10, 55, 66, 99]\n",
    "test_on_dataset(X_breast_cancer,y_breast_cancer,seed_list=seeds,criterion=\"gini\",max_depth=5,min_sample_split=2,alpha=0.0,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd6819",
   "metadata": {},
   "source": [
    "#### Test 5.2 Entropy Impurity\n",
    "\n",
    "- Test on multiple seeds\n",
    "- `max_depth=5`\n",
    "- `criterion='entropy'`\n",
    "- `alpha=0`: No pruning\n",
    "\n",
    "We reached exact same accuracy as Sklearn results on these seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6a8c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [0, 1, 5, 9, 30]\n",
      "criterion = entropy max_depth = None min_sample_split = 2 alpha = 0.3\n",
      "==============================================\n",
      "\n",
      "--- Seed = 0 ---\n",
      "[OUR CART] acc=0.9667, loss=0.0333\n",
      "[SK CART ] acc=0.9667, loss=0.0333\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 1 ---\n",
      "[OUR CART] acc=0.9667, loss=0.0333\n",
      "[SK CART ] acc=0.9667, loss=0.0333\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 5 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 9 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 30 ---\n",
      "[OUR CART] acc=0.9000, loss=0.1000\n",
      "[SK CART ] acc=0.9000, loss=0.1000\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "seeds = [0, 1, 5, 9, 30]\n",
    "test_on_dataset(X_iris,y_iris,seed_list=seeds,criterion=\"entropy\",max_depth=None,min_sample_split=2,alpha=0.3,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f96f3b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [3, 6, 8, 25, 88]\n",
      "criterion = entropy max_depth = 5 min_sample_split = 2 alpha = 0.0\n",
      "==============================================\n",
      "\n",
      "--- Seed = 3 ---\n",
      "[OUR CART] acc=0.9240, loss=0.0760\n",
      "[SK CART ] acc=0.9240, loss=0.0760\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MISMATCH] 4 samples have different predictions:\n",
      "    test_idx=5: y_true=1, y_our=1, y_sk=0\n",
      "    test_idx=32: y_true=0, y_our=0, y_sk=1\n",
      "    test_idx=108: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=140: y_true=1, y_our=0, y_sk=1\n",
      "\n",
      "--- Seed = 6 ---\n",
      "[OUR CART] acc=0.9649, loss=0.0351\n",
      "[SK CART ] acc=0.9649, loss=0.0351\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 8 ---\n",
      "[OUR CART] acc=0.9123, loss=0.0877\n",
      "[SK CART ] acc=0.9123, loss=0.0877\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 25 ---\n",
      "[OUR CART] acc=0.9357, loss=0.0643\n",
      "[SK CART ] acc=0.9357, loss=0.0643\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 88 ---\n",
      "[OUR CART] acc=0.9532, loss=0.0468\n",
      "[SK CART ] acc=0.9532, loss=0.0468\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MISMATCH] 4 samples have different predictions:\n",
      "    test_idx=63: y_true=0, y_our=1, y_sk=0\n",
      "    test_idx=80: y_true=0, y_our=0, y_sk=1\n",
      "    test_idx=98: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=108: y_true=1, y_our=1, y_sk=0\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_breast_cancer, y_breast_cancer = load_breast_cancer(return_X_y=True)\n",
    "seeds = [3,6,8,25,88]\n",
    "test_on_dataset(X_breast_cancer,y_breast_cancer,seed_list=seeds,criterion=\"entropy\",max_depth=5,min_sample_split=2,alpha=0.0,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a837d",
   "metadata": {},
   "source": [
    "#### Test 5.3 Gini Impurity with Pruning\n",
    "\n",
    "- Test on multiple seeds\n",
    "- `max_depth=10`\n",
    "- `criterion='gini'`\n",
    "- `alpha=0.3`: Pruning Applied\n",
    "\n",
    "We reached exact same accuracy as Sklearn results on these seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3834bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [2, 3, 5, 10, 13]\n",
      "criterion = gini max_depth = 10 min_sample_split = 2 alpha = 0.3\n",
      "==============================================\n",
      "\n",
      "--- Seed = 2 ---\n",
      "[OUR CART] acc=0.9778, loss=0.0222\n",
      "[SK CART ] acc=0.9778, loss=0.0222\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 3 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 5 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 10 ---\n",
      "[OUR CART] acc=1.0000, loss=0.0000\n",
      "[SK CART ] acc=1.0000, loss=0.0000\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 13 ---\n",
      "[OUR CART] acc=0.9778, loss=0.0222\n",
      "[SK CART ] acc=0.9778, loss=0.0222\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iris dataset\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "seeds = [2, 3, 5, 10, 13]\n",
    "test_on_dataset(X_iris,y_iris,seed_list=seeds,criterion=\"gini\",max_depth=10,min_sample_split=2,alpha=0.3,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c66648b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [1, 6, 7, 10, 13]\n",
      "criterion = gini max_depth = 10 min_sample_split = 2 alpha = 0.3\n",
      "==============================================\n",
      "\n",
      "--- Seed = 1 ---\n",
      "[OUR CART] acc=0.9415, loss=0.0585\n",
      "[SK CART ] acc=0.9415, loss=0.0585\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MISMATCH] 2 samples have different predictions:\n",
      "    test_idx=126: y_true=1, y_our=1, y_sk=0\n",
      "    test_idx=165: y_true=0, y_our=1, y_sk=0\n",
      "\n",
      "--- Seed = 6 ---\n",
      "[OUR CART] acc=0.9649, loss=0.0351\n",
      "[SK CART ] acc=0.9649, loss=0.0351\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 7 ---\n",
      "[OUR CART] acc=0.9532, loss=0.0468\n",
      "[SK CART ] acc=0.9532, loss=0.0468\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MISMATCH] 2 samples have different predictions:\n",
      "    test_idx=96: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=131: y_true=0, y_our=0, y_sk=1\n",
      "\n",
      "--- Seed = 10 ---\n",
      "[OUR CART] acc=0.9415, loss=0.0585\n",
      "[SK CART ] acc=0.9415, loss=0.0585\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MISMATCH] 2 samples have different predictions:\n",
      "    test_idx=43: y_true=0, y_our=1, y_sk=0\n",
      "    test_idx=157: y_true=1, y_our=1, y_sk=0\n",
      "\n",
      "--- Seed = 13 ---\n",
      "[OUR CART] acc=0.9181, loss=0.0819\n",
      "[SK CART ] acc=0.9181, loss=0.0819\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# breast cancer dataset\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_breast_cancer, y_breast_cancer = load_breast_cancer(return_X_y=True)\n",
    "seeds = [1,6,7,10,13]\n",
    "test_on_dataset(X_breast_cancer,y_breast_cancer,seed_list=seeds,criterion=\"gini\",max_depth=10,min_sample_split=2,alpha=0.3,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de3240",
   "metadata": {},
   "source": [
    "#### Test 5.4 Entropy Impurity with Pruning\n",
    "\n",
    "- Test on multiple seeds\n",
    "- `max_depth=10`\n",
    "- `criterion='entropy'`\n",
    "- `alpha=0.3`: Pruning Applied\n",
    "\n",
    "We reached exact same accuracy as Sklearn results on these seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "981bbe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [2, 3, 5, 10, 13]\n",
      "criterion = entropy max_depth = 10 min_sample_split = 2 alpha = 0.3\n",
      "==============================================\n",
      "\n",
      "--- Seed = 2 ---\n",
      "[OUR CART] acc=0.9778, loss=0.0222\n",
      "[SK CART ] acc=0.9778, loss=0.0222\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 3 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 5 ---\n",
      "[OUR CART] acc=0.9333, loss=0.0667\n",
      "[SK CART ] acc=0.9333, loss=0.0667\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 10 ---\n",
      "[OUR CART] acc=1.0000, loss=0.0000\n",
      "[SK CART ] acc=1.0000, loss=0.0000\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 13 ---\n",
      "[OUR CART] acc=0.9556, loss=0.0444\n",
      "[SK CART ] acc=0.9556, loss=0.0444\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iris dataset\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "seeds = [2, 3, 5, 10, 13]\n",
    "test_on_dataset(X_iris,y_iris,seed_list=seeds,criterion=\"entropy\",max_depth=10,min_sample_split=2,alpha=0.3,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e97ff7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Testing on dataset with seeds: [3, 6, 21, 42]\n",
      "criterion = entropy max_depth = 10 min_sample_split = 2 alpha = 0.3\n",
      "==============================================\n",
      "\n",
      "--- Seed = 3 ---\n",
      "[OUR CART] acc=0.9240, loss=0.0760\n",
      "[SK CART ] acc=0.9240, loss=0.0760\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MISMATCH] 4 samples have different predictions:\n",
      "    test_idx=5: y_true=1, y_our=1, y_sk=0\n",
      "    test_idx=32: y_true=0, y_our=0, y_sk=1\n",
      "    test_idx=108: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=140: y_true=1, y_our=0, y_sk=1\n",
      "\n",
      "--- Seed = 6 ---\n",
      "[OUR CART] acc=0.9649, loss=0.0351\n",
      "[SK CART ] acc=0.9649, loss=0.0351\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MATCH] our predictions == sklearn predictions on all test samples.\n",
      "\n",
      "--- Seed = 21 ---\n",
      "[OUR CART] acc=0.9064, loss=0.0936\n",
      "[SK CART ] acc=0.9064, loss=0.0936\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = +0.0000\n",
      "  [MISMATCH] 2 samples have different predictions:\n",
      "    test_idx=132: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=150: y_true=1, y_our=1, y_sk=0\n",
      "\n",
      "--- Seed = 42 ---\n",
      "[OUR CART] acc=0.9474, loss=0.0526\n",
      "[SK CART ] acc=0.9474, loss=0.0526\n",
      "Diff: acc (our - sk) = +0.0000, loss (our - sk) = -0.0000\n",
      "  [MISMATCH] 4 samples have different predictions:\n",
      "    test_idx=13: y_true=0, y_our=1, y_sk=0\n",
      "    test_idx=17: y_true=1, y_our=0, y_sk=1\n",
      "    test_idx=30: y_true=1, y_our=1, y_sk=0\n",
      "    test_idx=104: y_true=1, y_our=1, y_sk=0\n",
      "\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# breast cancer dataset\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "X_breast_cancer, y_breast_cancer = load_breast_cancer(return_X_y=True)\n",
    "seeds = [3,6,21,42]\n",
    "test_on_dataset(X_breast_cancer,y_breast_cancer,seed_list=seeds,criterion=\"entropy\",max_depth=10,min_sample_split=2,alpha=0.3,test_size=0.3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021a9b7-685b-4b7d-bcb8-90746cfd3e7b",
   "metadata": {},
   "source": [
    "### Test 6: Node impurity calculation\n",
    "\n",
    "Finally, we directly unit-test our impurity functions `node_score_gini` and `node_score_entropy` against sklearn’s impurity values on several label distributions:\n",
    "\n",
    "- pure node (all labels identical),\n",
    "- balanced 50/50 node,\n",
    "- skewed binary labels,\n",
    "- multi-class labels.\n",
    "\n",
    "We construct a root-only sklearn tree and compare the impurity stored at the root to our implementation (up to a log-base factor for entropy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bc9a343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- pure node ---\n",
      "  Gini:    sklearn=0.000000, ours=0.000000\n",
      "  Entropy: sklearn=0.000000, ours=0.000000\n",
      "--- balanced 50/50 ---\n",
      "  Gini:    sklearn=0.500000, ours=0.500000\n",
      "  Entropy: sklearn=1.000000, ours=1.000000\n",
      "--- skewed binary ---\n",
      "  Gini:    sklearn=0.375000, ours=0.375000\n",
      "  Entropy: sklearn=0.811278, ours=0.811278\n",
      "--- multi-class ---\n",
      "  Gini:    sklearn=0.666667, ours=0.666667\n",
      "  Entropy: sklearn=1.584963, ours=1.584963\n",
      "\n",
      "All impurity tests PASSED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def test_node_impurity_against_sklearn():\n",
    "    \"\"\"\n",
    "    Unit-test node_score_gini_from_counts and node_score_entropy_from_counts\n",
    "    against sklearn's root impurity on several label distributions.\n",
    "    \"\"\"\n",
    "    def sklearn_root_impurity(y, criterion):\n",
    "        \"\"\"\n",
    "        Fit a root-only sklearn tree and return its root impurity.\n",
    "        \"\"\"\n",
    "        X_dummy = np.zeros((len(y), 1))  # dummy feature\n",
    "        clf = DecisionTreeClassifier(\n",
    "            criterion=criterion,\n",
    "            max_depth=1,          # root only\n",
    "            random_state=0\n",
    "        )\n",
    "        clf.fit(X_dummy, y)\n",
    "        return float(clf.tree_.impurity[0])\n",
    "\n",
    "    def our_impurity(y, criterion):\n",
    "        \"\"\"\n",
    "        Compute impurity using our counts-based implementation.\n",
    "        \"\"\"\n",
    "        classes = np.unique(y)\n",
    "        mapping = {c: i for i, c in enumerate(classes)}\n",
    "        y_enc = np.array([mapping[c] for c in y], dtype=np.int64)\n",
    "        counts = np.bincount(y_enc, minlength=len(classes))\n",
    "\n",
    "        if criterion == \"gini\":\n",
    "            return node_score_gini_from_counts(counts)\n",
    "        elif criterion == \"entropy\":\n",
    "            return node_score_entropy_from_counts(counts)\n",
    "\n",
    "    # Test cases: (description, labels)\n",
    "    test_cases = [\n",
    "        (\"pure node\",        np.array([0, 0, 0, 0, 0])),\n",
    "        (\"balanced 50/50\",   np.array([0, 0, 1, 1])),\n",
    "        (\"skewed binary\",    np.array([0, 0, 0, 1])),\n",
    "        (\"multi-class\",      np.array([0, 1, 2, 0, 1, 2])),\n",
    "    ]\n",
    "    \n",
    "    for name, y in test_cases:\n",
    "        print(f\"--- {name} ---\")\n",
    "        # Gini\n",
    "        sk_gini = sklearn_root_impurity(y, criterion=\"gini\")\n",
    "        our_gini = our_impurity(y, criterion=\"gini\")\n",
    "        print(f\"  Gini:    sklearn={sk_gini:.6f}, ours={our_gini:.6f}\")\n",
    "        assert np.isclose(our_gini, sk_gini, atol=1e-12), f\"Gini mismatch for case '{name}'\"\n",
    "\n",
    "        # Entropy\n",
    "        sk_entropy = sklearn_root_impurity(y, criterion=\"entropy\")\n",
    "        our_entropy = our_impurity(y, criterion=\"entropy\")\n",
    "        print(f\"  Entropy: sklearn={sk_entropy:.6f}, ours={our_entropy:.6f}\")\n",
    "        assert np.isclose(our_entropy, sk_entropy, atol=1e-12), f\"Entropy mismatch for case '{name}'\"\n",
    "\n",
    "    print(\"\\nAll impurity tests PASSED\")\n",
    "    \n",
    "test_node_impurity_against_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c7d8b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. scikit-learn developers (2024) *Decision Trees: Mathematical Formulation*. Available at: https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation.\n",
    "\n",
    "2. Breiman, L., Friedman, J., Olshen, R. and Stone, C., 1984. Classification and Regression Trees. Belmont, CA: Wadsworth.\n",
    "\n",
    "3. Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.\n",
    "\n",
    "4. Irizarry, R., 2023. Data Science: Decision Trees (Section 11). Harvard T.H. Chan School of Public Health. Available at: https://rafalab.dfci.harvard.edu/pages/649/section-11.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0f1ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
