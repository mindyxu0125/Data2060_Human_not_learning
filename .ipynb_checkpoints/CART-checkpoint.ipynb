{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c95da0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de8de31-4d21-49ed-a953-18561f05383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def node_score_gini(probs):\n",
    "    '''\n",
    "    Compute Gini impurity for a probability vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs: 1D numpy array\n",
    "           Class probabilities p_k for a node, summing to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Gini impurity G = 1 - sum_k p_k^2.\n",
    "    '''\n",
    "    if probs.size == 0:\n",
    "        return 0.0\n",
    "    return 1.0 - np.sum(probs ** 2)\n",
    "\n",
    "\n",
    "def node_score_entropy(probs):\n",
    "    '''\n",
    "    Compute Entropy impurity for a probability vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs: 1D numpy array\n",
    "        Class probabilities p_k for a node, summing to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Entropy H = - sum_k p_k log(p_k) with the convention 0 * log 0 = 0.\n",
    "    '''\n",
    "    if probs.size == 0:\n",
    "        return 0.0\n",
    "    mask = probs > 0.0\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    p = probs[mask]\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "def _class_counts(y, n_classes):\n",
    "    '''\n",
    "    Count how many examples of each class appear in y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1D numpy array, shape (n_samples,)\n",
    "        Class labels for the samples in a node.\n",
    "    n_classes: int\n",
    "        Total number of distinct classes in the task.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts: 1D numpy array, shape (n_classes,)\n",
    "        counts[k] = number of samples of class k.\n",
    "    '''\n",
    "    return np.bincount(y, minlength=n_classes)\n",
    "\n",
    "\n",
    "def _to_probs(counts):\n",
    "    '''\n",
    "    Convert class counts to probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts: 1D numpy array\n",
    "        Class counts at a node.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    probs: 1D numpy array\n",
    "        Class probabilities p_k = counts[k] / sum(counts).\n",
    "        Returns all zeros if the node is empty.\n",
    "    '''\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return np.zeros_like(counts, dtype=float)\n",
    "    return counts.astype(float) / float(total)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper structure representing a single node in the CART tree.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    depth: int\n",
    "        Depth of the node (root has depth 0).\n",
    "    is_leaf: bool\n",
    "        Whether this node is a leaf.\n",
    "    feature_index: int or None\n",
    "        Index of feature used to split at this node (None for leaves).\n",
    "    threshold: float or None\n",
    "        Threshold value t for the split x_f <= t (None for leaves).\n",
    "    left: Node or None\n",
    "        Left child (samples with x_f <= t).\n",
    "    right: Node or None\n",
    "        Right child (samples with x_f > t).\n",
    "    class_counts : 1D numpy array\n",
    "        Counts of each class for samples reaching this node.\n",
    "    proba: 1D numpy array\n",
    "        Empirical class probability vector at this node.\n",
    "    prediction: int\n",
    "        Predicted class label at this node (argmax of 'proba').\n",
    "    n_samples: int\n",
    "        Number of samples that reached this node.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, depth, class_counts):\n",
    "        self.depth = depth\n",
    "        self.is_leaf = True    \n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        self.class_counts = class_counts.astype(int)\n",
    "        self.n_samples = int(class_counts.sum())\n",
    "\n",
    "        self.proba = _to_probs(self.class_counts)\n",
    "        self.prediction = int(np.argmax(self.proba))\n",
    "\n",
    "\n",
    "class DecisionTreeCART:\n",
    "    '''\n",
    "    CART (Classification and Regression Tree) classifier implemented from scratch.\n",
    "\n",
    "    Representation (project definition):\n",
    "\n",
    "        - Domain:   each sample X_i is an n-dimensional feature vector\n",
    "        - Labels:   Y = {0, 1, ..., K-1}\n",
    "        - Training data: D = {(x_i, y_i)}_{i=1}^N\n",
    "        - Output:  a binary decision tree of depth at most 'max_depth'\n",
    "                   Each leaf stores an empirical class probability vector,\n",
    "                   and predictions are argmax_k p_k at the leaf.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth: int or None\n",
    "        Maximum depth of the tree (root has depth 0). If None, the tree \n",
    "        can grow until all leaves are pure or no further split improves impurity.\n",
    "    min_samples_split: int\n",
    "        Minimum number of samples required at a node to consider splitting it.\n",
    "    impurity: str\n",
    "        Impurity measure to minimize at each split. Either 'gini' or 'entropy.'\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, impurity='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = max(min_samples_split, 2)\n",
    "        self.impurity_name = impurity\n",
    "        self.n_classes_ = None\n",
    "        self.n_features_ = None\n",
    "        self.root_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Train the CART classifier on labeled data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix.\n",
    "        y: array-like of shape (n_samples,)\n",
    "            Training labels in {0, 1, ..., K-1}.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: DecisionTreeCART\n",
    "            Fitted estimator.\n",
    "        '''\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_features_ = n_features\n",
    "        self.n_classes_ = int(np.max(y)) + 1\n",
    "\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        self.root_ = self._build_tree(X, y, indices, depth=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict class labels for a matrix of input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: numpy array of shape (n_samples,)\n",
    "            Predicted class label for each sample.\n",
    "        '''\n",
    "        X = np.asarray(X)\n",
    "        preds = [self._predict_one(x, self.root_) for x in X]\n",
    "        return np.array(preds, dtype=int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Predict class probabilities for each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba: numpy array of shape (n_samples, n_classes)\n",
    "            probas[i, k] = estimated probability of class k for sample i.\n",
    "        '''\n",
    "        X = np.asarray(X)\n",
    "        probas = [self._predict_proba_one(x, self.root_) for x in X]\n",
    "        return np.vstack(probas)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute classification accuracy on a dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Fraction of correct predictions (between 0 and 1).\n",
    "        '''\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        '''\n",
    "        Compute misclassification loss on a dataset.\n",
    "\n",
    "        Loss is defined as the fraction of incorrectly classified samples:\n",
    "\n",
    "            loss = 1 - accuracy\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Misclassification rate (between 0 and 1).\n",
    "        '''\n",
    "        return 1.0 - self.accuracy(X, y)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Tree construction\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def _build_tree(self, X, y, indices, depth):\n",
    "        '''\n",
    "        Recursively grow the tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array, shape (n_samples, n_features)\n",
    "            Full training feature matrix.\n",
    "        y: numpy array, shape (n_samples,)\n",
    "            Full training label vector.\n",
    "        indices: 1D numpy array\n",
    "            Indices of training samples that reach this node.\n",
    "        depth: int\n",
    "            Depth of the current node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        node : Node\n",
    "            The constructed (sub)tree root for this subset.\n",
    "        '''\n",
    "        y_node = y[indices]\n",
    "        counts = _class_counts(y_node, self.n_classes_)\n",
    "        node = Node(depth=depth, class_counts=counts)\n",
    "\n",
    "        # Stopping criteria\n",
    "        if self._is_terminal(node, indices, y_node):\n",
    "            return node\n",
    "\n",
    "        # Find best split using our optimizer\n",
    "        best_feature, best_threshold, best_gain = self._best_split(X, y_node, indices)\n",
    "\n",
    "        if best_feature is None or best_gain <= 0.0:\n",
    "            # No split improves impurity -> keep as leaf\n",
    "            return node\n",
    "\n",
    "        # Turn node into an internal split node\n",
    "        node.is_leaf = False\n",
    "        node.feature_index = best_feature\n",
    "        node.threshold = best_threshold\n",
    "\n",
    "        feature_values = X[indices, best_feature]\n",
    "        left_mask = feature_values <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_indices = indices[left_mask]\n",
    "        right_indices = indices[right_mask]\n",
    "\n",
    "        node.left = self._build_tree(X, y, left_indices, depth + 1)\n",
    "        node.right = self._build_tree(X, y, right_indices, depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _is_terminal(self, node, indices, y_node):\n",
    "        '''\n",
    "        Check whether a node should stop splitting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node: Node\n",
    "            Current node.\n",
    "        indices: 1D numpy array\n",
    "            Indices of samples reaching this node.\n",
    "        y_node: 1D numpy array\n",
    "            Labels of samples at this node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if the node should remain a leaf, False otherwise.\n",
    "\n",
    "        Stopping rules:\n",
    "\n",
    "            1. Node is empty (no samples).\n",
    "            2. All samples at the node share the same class label.\n",
    "            3. Node depth reached max_depth (if max_depth is set).\n",
    "            4. Number of samples is smaller than 'min_samples_split'.\n",
    "        '''\n",
    "        n_samples = indices.size\n",
    "\n",
    "        # 1. empty node\n",
    "        if n_samples == 0:\n",
    "            return True\n",
    "\n",
    "        # 2. pure node (all same label)\n",
    "        if np.unique(y_node).size == 1:\n",
    "            return True\n",
    "\n",
    "        # 3. max depth reached\n",
    "        if self.max_depth is not None and node.depth >= self.max_depth:\n",
    "            return True\n",
    "\n",
    "        # 4. not enough samples to split further\n",
    "        if n_samples < self.min_samples_split:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _best_split(self, X, y_node, indices):\n",
    "        '''\n",
    "        Find the best (feature, threshold) split for a node.\n",
    "\n",
    "        Follows the project pseudo-code:\n",
    "\n",
    "            best_gain <- 0\n",
    "            best_feature, best_threshold <- None\n",
    "\n",
    "            for each feature f in F:\n",
    "                for each possible threshold t in f:\n",
    "                    Split S into S_left and S_right using (f, t)\n",
    "                    if either split is empty: continue\n",
    "                    gain <- Impurity(S)\n",
    "                         - (|S_left| / |S|) * Impurity(S_left)\n",
    "                         - (|S_right| / |S|) * Impurity(S_right)\n",
    "                    if gain > best_gain:\n",
    "                        best_gain <- gain\n",
    "                        best_feature <- f\n",
    "                        best_threshold <- t\n",
    "\n",
    "            return (best_feature, best_threshold)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array, shape (n_samples, n_features)\n",
    "            Full feature matrix.\n",
    "        y_node: numpy array, shape (n_node_samples,)\n",
    "            Labels of samples reaching this node.\n",
    "        indices: 1D numpy array\n",
    "            Indices of samples forming the current dataset S.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        best_feature: int or None\n",
    "        best_threshold: float or None\n",
    "        best_gain: float\n",
    "        '''\n",
    "        n_node_samples = indices.size\n",
    "        if n_node_samples == 0:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        # Parent impurity\n",
    "        parent_counts = _class_counts(y_node, self.n_classes_)\n",
    "        parent_probs = _to_probs(parent_counts)\n",
    "        parent_impurity = self._impurity(parent_probs)\n",
    "\n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # Iterate over all features f in F\n",
    "        for f in range(self.n_features_):\n",
    "            feature_values = X[indices, f]\n",
    "\n",
    "            unique_vals = np.unique(feature_values)\n",
    "            if unique_vals.size <= 1:\n",
    "                # No meaningful split on this feature\n",
    "                continue\n",
    "\n",
    "            # Candidate thresholds: midpoints between unique sorted values\n",
    "            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n",
    "\n",
    "            for t in thresholds:\n",
    "                left_mask = feature_values <= t\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if not np.any(left_mask) or not np.any(right_mask):\n",
    "                    continue\n",
    "\n",
    "                left_indices = indices[left_mask]\n",
    "                right_indices = indices[right_mask]\n",
    "\n",
    "                y_left = y_node[left_mask]\n",
    "                y_right = y_node[right_mask]\n",
    "\n",
    "                left_counts = _class_counts(y_left, self.n_classes_)\n",
    "                right_counts = _class_counts(y_right, self.n_classes_)\n",
    "\n",
    "                left_probs = _to_probs(left_counts)\n",
    "                right_probs = _to_probs(right_counts)\n",
    "\n",
    "                impur_left = self._impurity(left_probs)\n",
    "                impur_right = self._impurity(right_probs)\n",
    "\n",
    "                w_left = float(left_indices.size) / float(n_node_samples)\n",
    "                w_right = float(right_indices.size) / float(n_node_samples)\n",
    "\n",
    "                gain = parent_impurity - w_left * impur_left - w_right * impur_right\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = f\n",
    "                    best_threshold = t\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _impurity(self, probs):\n",
    "        '''\n",
    "        Dispatch to the chosen impurity function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probs: 1D numpy array\n",
    "            Class probability vector for a node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Impurity value according to self.impurity_name.\n",
    "        '''\n",
    "        if self.impurity_name == 'gini':\n",
    "            return node_score_gini(probs)\n",
    "        else:\n",
    "            return node_score_entropy(probs)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        '''\n",
    "        Predict class label for a single sample by traversing the tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:1D numpy array of shape (n_features,)\n",
    "            Feature vector for one sample.\n",
    "        node: Node\n",
    "            Current node in the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Predicted class label.\n",
    "        '''\n",
    "        while not node.is_leaf:\n",
    "            f = node.feature_index\n",
    "            t = node.threshold\n",
    "            if x[f] <= t:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def _predict_proba_one(self, x, node):\n",
    "        '''\n",
    "        Predict class probabilities for a single sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: 1D numpy array of shape (n_features,)\n",
    "            Feature vector for one sample.\n",
    "        node:Node\n",
    "            Current node in the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: 1D numpy array of shape (n_classes,)\n",
    "            Empirical class probabilities stored at the leaf.\n",
    "        '''\n",
    "        while not node.is_leaf:\n",
    "            f = node.feature_index\n",
    "            t = node.threshold\n",
    "            if x[f] <= t:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1094318",
   "metadata": {},
   "source": [
    "## Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b460de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (398, 30)\n",
      "X_test shape: (171, 30)\n",
      "Class counts: [212 357]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pytest\n",
    "\n",
    "# breast_cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # 0/1\n",
    "\n",
    "# split train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=0,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Class counts:\", np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5393d-e113-4e83-8634-b00f68f05e2a",
   "metadata": {},
   "source": [
    "### One update: Comment src.cart import DecisionTreeCART (pls correct it if needed, if not, just delete this and the comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a810fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.cart import DecisionTreeCART\n",
    "\n",
    "def make_cart_model(max_depth=5, min_samples_split=2, impurity='gini'):\n",
    "    '''\n",
    "    Helper function to initialize our CART model.\n",
    "    '''\n",
    "    model = DecisionTreeCART(max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        impurity=impurity\n",
    "        )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c0809",
   "metadata": {},
   "source": [
    "### Test 1 — train() runs correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54ce17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: train() runs without error.\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "model = make_cart_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test 1 passed: train() runs without error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d7aa3",
   "metadata": {},
   "source": [
    "### Test 2 — predict() shape & class values check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b8d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 passed: predict() shape & value checks passed. Train accuracy = 0.997\n",
      "Test accuracy = 0.918\n"
     ]
    }
   ],
   "source": [
    "# Test 2: predict() should produce outputs with correct shape and valid class values\n",
    "\n",
    "model = make_cart_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Check output shape\n",
    "assert y_pred_train.shape == y_train.shape, \\\n",
    "    f\"Prediction shape mismatch: y_pred shape={y_pred_train.shape}, y_train shape={y_train.shape}\"\n",
    "\n",
    "# Check value range (breast_cancer is a binary classification dataset)\n",
    "unique_vals = np.unique(y_pred_train)\n",
    "assert set(unique_vals).issubset({0, 1}), \\\n",
    "    f\"Predicted values must be 0/1. Found values: {unique_vals}\"\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Test 2 passed: predict() shape & value checks passed. Train accuracy = {train_acc:.3f}\")\n",
    "\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "print(f\"Test accuracy = {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224f253",
   "metadata": {},
   "source": [
    "### Test 3 — loss() returns a finite scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a06194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 passed: loss() returns a valid finite scalar. Train loss = 0.002513\n"
     ]
    }
   ],
   "source": [
    "# Test 3: loss() should return a finite scalar value\n",
    "\n",
    "model = make_cart_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_loss = model.loss(X_train, y_train)\n",
    "\n",
    "assert np.isscalar(train_loss), \"loss() should return a scalar value.\"\n",
    "assert np.isfinite(train_loss), \"loss() should not return NaN or infinity.\"\n",
    "\n",
    "print(f\"Test 3 passed: loss() returns a valid finite scalar. Train loss = {train_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747a13e",
   "metadata": {},
   "source": [
    "Our loss function is defined as the misclassification error rate, therefore it should be a scalar between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64412045",
   "metadata": {},
   "source": [
    "### Test 4: Edge case testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5eb1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_toy:\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "y_toy: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# A small toy dataset for edge case testing\n",
    "X_toy = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "])\n",
    "y_toy = np.array([0, 0, 1, 1])\n",
    "\n",
    "print(\"X_toy:\\n\", X_toy)\n",
    "print(\"y_toy:\", y_toy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c918c27",
   "metadata": {},
   "source": [
    "#### Test 4.1 — Edge case: all labels are the same (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cb7275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.1 passed: all-zero labels edge case handled correctly.\n",
      "Predicted labels: [0 0 0 0]\n",
      "Loss on all-zero labels: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test 4.1: all labels are zero (only one class present)\n",
    "\n",
    "model_zero = make_cart_model()\n",
    "\n",
    "y_all_zero = np.zeros_like(y_toy)\n",
    "model_zero.fit(X_toy, y_all_zero)\n",
    "\n",
    "y_pred_zero = model_zero.predict(X_toy)\n",
    "loss_zero = model_zero.loss(X_toy, y_all_zero)\n",
    "\n",
    "assert y_pred_zero.shape == y_all_zero.shape\n",
    "assert np.isfinite(loss_zero)\n",
    "\n",
    "print(\"Test 4.1 passed: all-zero labels edge case handled correctly.\")\n",
    "print(\"Predicted labels:\", y_pred_zero)\n",
    "print(\"Loss on all-zero labels:\", loss_zero)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cae04",
   "metadata": {},
   "source": [
    "#### Test 4.2 — Edge case: single feature only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdeffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.2 passed: single-feature edge case handled correctly.\n",
      "Predicted labels: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test 4.2: dataset contains only one feature\n",
    "\n",
    "model_single = make_cart_model()\n",
    "\n",
    "X_single = X_toy[:, :1]  # Use only the first feature\n",
    "model_single.fit(X_single, y_toy)\n",
    "\n",
    "y_pred_single = model_single.predict(X_single)\n",
    "assert y_pred_single.shape == y_toy.shape\n",
    "\n",
    "print(\"Test 4.2 passed: single-feature edge case handled correctly.\")\n",
    "\n",
    "assert np.array_equal(y_pred_single, y_toy)\n",
    "print(\"Predicted labels:\", y_pred_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594a1c7",
   "metadata": {},
   "source": [
    "#### Test 4.3 — Edge case: all-zero features X=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f302c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.3 passed: all-zero features edge case handled correctly.\n",
      "Predicted labels: [0 0 0 0]\n",
      "Loss on all-zero features: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Test 4.3: all feature values are zero\n",
    "\n",
    "model_feat_zero = make_cart_model()\n",
    "\n",
    "X_zeros = np.zeros_like(X_toy)\n",
    "model_feat_zero.fit(X_zeros, y_toy)\n",
    "\n",
    "y_pred_zeros = model_feat_zero.predict(X_zeros)\n",
    "loss_zeros = model_feat_zero.loss(X_zeros, y_toy)\n",
    "\n",
    "assert y_pred_zeros.shape == y_toy.shape\n",
    "assert np.isfinite(loss_zeros)\n",
    "\n",
    "print(\"Test 4.3 passed: all-zero features edge case handled correctly.\")\n",
    "print(\"Predicted labels:\", y_pred_zeros)\n",
    "print(\"Loss on all-zero features:\", loss_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da70f4",
   "metadata": {},
   "source": [
    "### (Updated--more strict thresholds & add the case when impurity='entropy'& goal description)\n",
    "### Test 5: Reproduce sklearn’s DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78754ed4-50c7-45c4-8c0f-e10073b37df5",
   "metadata": {},
   "source": [
    "**（To be discussed/checked）其实我是set_random_seed=3的时候才会完全一样，不知道这个会不会是一个concern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7f5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn CART test accuracy: 0.918129\n",
      "Our CART test accuracy: 0.918129\n",
      "Same predictions as sklearn? False\n",
      "Absolute accuracy difference: 0.00000000\n",
      "Test 5 passed: our CART implementation matches sklearn CART (using 'gini' impurity).\n",
      "\n",
      "Sklearn CART test accuracy: 0.929825\n",
      "Our CART test accuracy: 0.929825\n",
      "Same predictions as sklearn? True\n",
      "Absolute accuracy difference: 0.00000000\n",
      "Test 5 passed: our CART implementation matches sklearn CART (using 'entropy' impurity).\n"
     ]
    }
   ],
   "source": [
    "# Test 5.1 Compare our predictions with sklearn's CART classifier\n",
    "# Sklearn CART (using Gini impurity)\n",
    "# Goal of this test:\n",
    "#   This test evaluates whether our CART implementation can\n",
    "#   successfully reproduce the behavior of sklearn’s\n",
    "#   DecisionTreeClassifier when trained on the same dataset\n",
    "#   with identical hyperparameters.\n",
    "\n",
    "sk_cart = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "sk_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = sk_cart.predict(X_test)\n",
    "sk_acc = accuracy_score(y_test, y_pred_sk)\n",
    "\n",
    "print(f\"Sklearn CART test accuracy: {sk_acc:.6f}\")\n",
    "\n",
    "# Our own CART implementation\n",
    "my_cart = make_cart_model()\n",
    "my_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_my = my_cart.predict(X_test)\n",
    "my_acc = accuracy_score(y_test, y_pred_my)\n",
    "\n",
    "print(f\"Our CART test accuracy: {my_acc:.6f}\")\n",
    "\n",
    "\n",
    "same_predictions = np.array_equal(y_pred_sk, y_pred_my)\n",
    "acc_diff = abs(sk_acc - my_acc)\n",
    "\n",
    "print(\"Same predictions as sklearn?\", same_predictions)\n",
    "print(f\"Absolute accuracy difference: {acc_diff:.8f}\")\n",
    "\n",
    "# Depending on implementation details, exact matching or near-matching are acceptable.\n",
    "assert my_acc == pytest.approx(sk_acc, abs=1e-12), \\\n",
    "    \"Our CART accuracy should be extremely close to sklearn's CART accuracy.\"\n",
    "\n",
    "print(\"Test 5 passed: our CART implementation matches sklearn CART (using 'gini' impurity).\\n\")\n",
    "\n",
    "#===============================================================================================================\n",
    "\n",
    "# Test 5.2 Compare our predictions with sklearn's CART classifier\n",
    "# Sklearn CART (using entropy impurity)\n",
    "\n",
    "sk_cart = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "sk_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = sk_cart.predict(X_test)\n",
    "sk_acc = accuracy_score(y_test, y_pred_sk)\n",
    "\n",
    "print(f\"Sklearn CART test accuracy: {sk_acc:.6f}\")\n",
    "\n",
    "# Our own CART implementation\n",
    "my_cart = make_cart_model(impurity='entropy')\n",
    "my_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_my = my_cart.predict(X_test)\n",
    "my_acc = accuracy_score(y_test, y_pred_my)\n",
    "\n",
    "print(f\"Our CART test accuracy: {my_acc:.6f}\")\n",
    "\n",
    "\n",
    "same_predictions = np.array_equal(y_pred_sk, y_pred_my)\n",
    "acc_diff = abs(sk_acc - my_acc)\n",
    "\n",
    "print(\"Same predictions as sklearn?\", same_predictions)\n",
    "print(f\"Absolute accuracy difference: {acc_diff:.8f}\")\n",
    "\n",
    "# Depending on implementation details, exact matching or near-matching are acceptable.\n",
    "assert my_acc == pytest.approx(sk_acc, abs=1e-12), \\\n",
    "    \"Our CART accuracy should be extremely close to sklearn's CART accuracy.\"\n",
    "\n",
    "print(\"Test 5 passed: our CART implementation matches sklearn CART (using 'entropy' impurity).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021a9b7-685b-4b7d-bcb8-90746cfd3e7b",
   "metadata": {},
   "source": [
    "### (Updated)Test 6: Node_score test (impurity_calculation test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47215a2e-513b-4234-9e15-bcc9097c649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impurity tests passed: our impurity calculations match sklearn exactly (Gini) and up to log-base (Entropy).\n"
     ]
    }
   ],
   "source": [
    "# Tests 6: Our CART impurity vs. sklearn impurity\n",
    "# Goal of this block:\n",
    "#   For several different label distributions, compare our\n",
    "#   node_score_gini and node_score_entropy against sklearn's\n",
    "#   impurity values at a root-only tree. This directly unit-tests\n",
    "#   our impurity functions and checks that they match sklearn\n",
    "#   (up to log-base for entropy).\n",
    "\n",
    "\n",
    "def sklearn_impurity(y, criterion):\n",
    "    '''\n",
    "    Compute sklearn impurity for labels y.\n",
    "    We force a ROOT-ONLY tree (no splits) by:\n",
    "      - Using a dummy constant feature X_dummy\n",
    "      - Setting min_samples_split > n_samples to prevent splitting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: array-like labels for the samples.\n",
    "    criterion : {'gini', 'entropy'}\n",
    "        Impurity measure to use internally in sklearn.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Impurity value stored at the root node.\n",
    "    '''\n",
    "    y = np.asarray(y)\n",
    "    X_dummy = np.zeros((len(y), 1))  \n",
    "\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=1,       \n",
    "        min_samples_split=len(y) + 1, \n",
    "        random_state=0\n",
    "    )\n",
    "    clf.fit(X_dummy, y)\n",
    "\n",
    "    return clf.tree_.impurity[0]\n",
    "    \n",
    "#===============================================================================================================\n",
    "# Test 6.1: Pure node (all labels identical)\n",
    "# Edge case: impurity should be zero for both gini and entropy.\n",
    "\n",
    "y_pure = [0, 0, 0, 0]\n",
    "sk_gini = sklearn_impurity(y_pure, 'gini')\n",
    "sk_entropy = sklearn_impurity(y_pure, 'entropy')\n",
    "\n",
    "n_classes = len(np.unique(y_pure))\n",
    "counts = _class_counts(np.array(y_pure), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "# Convert sklearn's base-2 entropy to natural log (based on our definition from the reference)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "#===============================================================================================================\n",
    "# Test 6.2: Balanced labels (50/50)\n",
    "# Edge case: maximal impurity for 2 classes (gini=0.5, entropy=1 bit).\n",
    "\n",
    "y_bal = [0, 1]\n",
    "sk_gini = sklearn_impurity(y_bal, 'gini')\n",
    "sk_entropy = sklearn_impurity(y_bal, 'entropy')\n",
    "\n",
    "n_classes = len(np.unique(y_bal))\n",
    "counts = _class_counts(np.array(y_bal), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "#===============================================================================================================\n",
    "# Test 6.3: Skewed labels (e.g., 2 zeros, 3 ones)\n",
    "# Regular case: impurity should lie strictly between 0 and the balanced-case maximum.\n",
    "\n",
    "y_skew = [0, 0, 1, 1, 1]\n",
    "sk_gini = sklearn_impurity(y_skew, 'gini')\n",
    "sk_entropy = sklearn_impurity(y_skew, 'entropy')\n",
    "\n",
    "n_classes = len(np.unique(y_skew))\n",
    "counts = _class_counts(np.array(y_skew), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "#===============================================================================================================\n",
    "# Test 6.4: Three-class labels\n",
    "# Multi-class case: checks that our impurity generalizes beyond binary labels.\n",
    "\n",
    "y_three = [0, 1, 2, 2, 2, 1]\n",
    "sk_gini = sklearn_impurity(y_three, \"gini\")\n",
    "sk_entropy = sklearn_impurity(y_three, \"entropy\")\n",
    "\n",
    "n_classes = len(np.unique(y_three))\n",
    "counts = _class_counts(np.array(y_three), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "print(\"Impurity tests passed: our impurity calculations match sklearn exactly (Gini) and up to log-base (Entropy).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58761419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# data = np.genfromtxt(\"breast_cancer.csv\", delimiter=\",\", dtype=str, skip_header=1)\n",
    "# diagnosis = data[:, 1]\n",
    "# X = data[:, 2:].astype(float)\n",
    "# y = (diagnosis == \"M\").astype(int)\n",
    "\n",
    "# print(\"X shape:\", X.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faadab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 1.0\n",
      "val acc: 0.9298245614035088\n",
      "test acc: 0.8245614035087719\n",
      "--- CART TREE ---\n",
      "[Feature 22 <= 105.1500] gain=0.3611\n",
      "\n",
      "  [Feature 24 <= 0.1759] gain=0.0592\n",
      "\n",
      "    [Feature 0 <= 14.9800] gain=0.0106\n",
      "\n",
      "      [Feature 27 <= 0.1807] gain=0.0108\n",
      "\n",
      "        [Feature 20 <= 15.7250] gain=0.0043\n",
      "\n",
      "          [Feature 12 <= 4.1055] gain=0.0058\n",
      "\n",
      "            [Feature 21 <= 33.1050] gain=0.0030\n",
      "\n",
      "              Leaf(label=1, samples=?)\n",
      "              [Feature 0 <= 12.0450] gain=0.3750\n",
      "\n",
      "                Leaf(label=1, samples=?)\n",
      "                Leaf(label=0, samples=?)\n",
      "            [Feature 0 <= 12.2650] gain=0.5000\n",
      "\n",
      "              Leaf(label=0, samples=?)\n",
      "              Leaf(label=1, samples=?)\n",
      "          [Feature 8 <= 0.1782] gain=0.3457\n",
      "\n",
      "            Leaf(label=1, samples=?)\n",
      "            Leaf(label=0, samples=?)\n",
      "        Leaf(label=0, samples=?)\n",
      "      Leaf(label=0, samples=?)\n",
      "    Leaf(label=0, samples=?)\n",
      "  [Feature 22 <= 114.4500] gain=0.0418\n",
      "\n",
      "    [Feature 1 <= 19.7100] gain=0.3174\n",
      "\n",
      "      [Feature 0 <= 14.1100] gain=0.3200\n",
      "\n",
      "        Leaf(label=0, samples=?)\n",
      "        Leaf(label=1, samples=?)\n",
      "      Leaf(label=0, samples=?)\n",
      "    [Feature 7 <= 0.0280] gain=0.0159\n",
      "\n",
      "      Leaf(label=1, samples=?)\n",
      "      Leaf(label=0, samples=?)\n",
      "--- END ---\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# random.seed(0)\n",
    "# n = len(y)\n",
    "# split1 = int(0.6 * n)\n",
    "# split2 = int(0.8 * n)\n",
    "\n",
    "# X_train, y_train = X[:split1], y[:split1]\n",
    "# X_valid, y_valid = X[split1:split2], y[split1:split2]\n",
    "# X_test, y_test = X[split2:], y[split2:]\n",
    "\n",
    "\n",
    "# from src.cart import DecisionTreeCART\n",
    "# tree = DecisionTreeCART(max_depth=40, min_samples_split=2)\n",
    "# tree.fit(X_train, y_train)\n",
    "\n",
    "# print(\"train acc:\", tree.accuracy(X_train, y_train))\n",
    "# print(\"val acc:\", tree.accuracy(X_valid, y_valid))\n",
    "# print(\"test acc:\", tree.accuracy(X_test, y_test))\n",
    "\n",
    "# tree.print_tree()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
