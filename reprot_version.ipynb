{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a0f62c",
   "metadata": {},
   "source": [
    "# DATA2060 Final Project\n",
    "\n",
    "Model: **CART for classification**\\\n",
    "Github repo: https://github.com/mindyxu0125/Data2060_Human_not_learning.git\n",
    "\n",
    "Team Members:\n",
    "- Muxin Fu\n",
    "- Yixiao Zhang\n",
    "- Jingmin Xu\n",
    "- Mingrui Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6573eaa",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a5121",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "### 0.1 Overview  \n",
    "The Classification and Regression Tree (CART) algorithm is a nonparametric supervised learning method that builds a binary decision tree for classification tasks. At each step, the algorithm selects a feature and threshold that create two child nodes with lower class impurity, using criteria such as Gini impurity or entropy. Through this recursive partitioning, CART represents the classifier as a set of piecewise-constant regions, where each leaf corresponds to a predicted class label. Because the sequence of splits directly mirrors the decision-making process, CART offers a transparent and intuitive model structure.\n",
    "### 0.2 Advantages  \n",
    "CART offers several notable strengths that contribute to its widespread use as a baseline classifier. First, the model is highly interpretable: each internal node corresponds to a clear “if–then” condition based on a single feature, allowing the entire decision path to be easily traced and communicated. This transparency is particularly valuable in settings where model explanations are required.  \n",
    "\n",
    "Second, CART is able to capture nonlinear relationships and feature interactions without relying on explicit transformations or parametric assumptions. Its recursive splitting procedure enables the model to adapt flexibly to irregular or complex decision boundaries, providing expressive power beyond that of linear models.  \n",
    "\n",
    "Moreover, CART requires minimal preprocessing. It can accommodate both numerical and categorical variables, is robust to monotonic feature scaling, and implicitly performs feature selection by choosing splits only on informative variables. These characteristics make CART convenient to implement and reliable across a wide range of practical applications.\n",
    "\n",
    "### 0.3 Disadvantages  \n",
    "Despite its advantages, CART also presents several limitations that must be considered. Most importantly, the model is prone to overfitting when allowed to grow without constraints. As emphasized in the bias–complexity trade-off discussed in the course reading, increasing model flexibility reduces approximation error but raises estimation error, causing deep, unpruned trees to exhibit high variance and poor generalization.  \n",
    "\n",
    "CART also tends to be unstable: small perturbations in the training data can alter early splits, resulting in substantially different tree structures. This sensitivity undermines the model’s reliability, especially in contexts requiring stable predictions.  \n",
    "\n",
    "Finally, because CART relies exclusively on axis-aligned splits, it may need many successive partitions to approximate diagonal or curved decision boundaries, leading to unnecessarily deep and complex trees. These shortcomings motivate the use of pruning techniques and more advanced ensemble methods, such as Random Forests and Gradient Boosting, which address variance and stability issues more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99340a",
   "metadata": {},
   "source": [
    "## 1. Representation\n",
    "### 1.1 **Domain Set**\n",
    "We define the domain space as  \n",
    "\n",
    "In the CART classification setting, each training example is represented as a feature vector in an $n$-dimensional real space:\n",
    "\n",
    "$$\n",
    "\\mathcal{X} = \\mathbb{R}^n, \\qquad x_i = (x_{i1}, x_{i2}, \\dots, x_{in}) \\in \\mathcal{X}.\n",
    "$$\n",
    "\n",
    "Each component $x_{ij}$ represents the value of feature $j$ for sample $i$.\n",
    "The feature domain can include continuous or categorical variables (encoded numerically in practice).\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 **Label Set**\n",
    "\n",
    "For a $K$-class classification task, the label space is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{Y} = \\{0, 1, \\dots, K-1\\}.\n",
    "$$\n",
    "\n",
    "In the binary case, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathcal{Y} = \\{0, 1\\}.\n",
    "$$\n",
    "\n",
    "### 1.3 **Training Data**\n",
    "\n",
    "We are given a labeled dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N},\n",
    "\\quad x_i \\in \\mathcal{X}, \\; y_i \\in \\mathcal{Y}.\n",
    "$$\n",
    "\n",
    "Each pair $(x_i, y_i)$ represents one training example.\n",
    "The training process recursively partitions $\\mathcal{D}$ based on feature thresholds to form a binary decision tree.\n",
    "\n",
    "### 1.4 **Learner's Output**\n",
    "Formally, the hypothesis space of CART classification is defined as the set of **binary decision trees** of depth at most $T_{\\max}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{H} =\n",
    "\\{\\, h : \\mathcal{X} \\to \\mathcal{Y} \\mid\n",
    "h \\text{ is a binary decision tree with depth } \\le T_{\\max} \\,\\}.\n",
    "$$\n",
    "\n",
    "Each decision tree $h \\in \\mathcal{H}$ recursively partitions the input space $\\mathcal{X}$ into at most $2^{T_{\\max}}$ disjoint leaves.\n",
    "\n",
    "At prediction time, a new observation $x$ is passed through the sequence of feature tests $(x_f \\le t)$ until it reaches a leaf node $i$.\n",
    "Each leaf stores an empirical class probability vector\n",
    "\n",
    "$$\n",
    "p_i = (p_{i,0}, p_{i,1}, \\dots, p_{i,K-1}),\n",
    "$$\n",
    "\n",
    "computed from the training samples that reached that leaf.\n",
    "\n",
    "The predicted class label is then determined by\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\arg\\max_k \\, p_{i,k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb1f67",
   "metadata": {},
   "source": [
    "## 2. Loss\n",
    "In the classification setting, losses are the **measures of impurity**.  CART minimizes impruity and the loss is defined per split. Generally speaking, **Gini** and **Entropy** are good measures.\n",
    "\n",
    "To compute Loss, we need: \n",
    "* Impurity measure, \n",
    "* Split loss based on chosen impurity measure.\n",
    "\n",
    "In the scikit-learn, this is determined by the parameter **criterion**: *{“gini”, “entropy”, “log_loss”}, default=”gini”* \n",
    "\n",
    "\n",
    "### 2.1 **Impurity Function**\n",
    "\n",
    "For a $K$-class classification problem, consider node $i$ containing a subset of samples\n",
    "\n",
    "$$S_i = \\{(x_j, y_j)\\}_{j \\in \\mathcal{I}_i}, \\qquad N_i = |S_i|.$$\n",
    "\n",
    "The number of samples in node $i$ that belong to class $k$ is\n",
    "\n",
    "$$n_{i,k} = \\sum_{j \\in \\mathcal{I}_i} \\mathbf{1}(y_j = k).$$\n",
    "\n",
    "The class proportion of class $k$ in node $i$ is\n",
    "\n",
    "$$p_{i,k} = \\frac{n_{i,k}}{N_i}, \\qquad k = 1, \\dots, K.$$\n",
    "$$\\sum_{k=1}^K p_{i,k} = 1,\\text{and  } p_{i,k} \\ge 0 \\quad \\text{for } k = 1, \\dots, K.$$\n",
    "\n",
    "#### 2.1.1 **Gini**\n",
    "\n",
    "\n",
    "- The Gini impurity of node $i$ is:   \n",
    "$$G_i = 1 - \\sum_{k=1}^K p_{i,k}^2.$$\n",
    "\n",
    "#### 2.1.2 **Entropy**\n",
    "\n",
    "- The entropy impurity of node $i$ is\n",
    "$$H_i = - \\sum_{k=1}^K p_{i,k} \\log p_{i,k},$$\n",
    "\n",
    "- And we assume $0 \\log 0 = 0$.\n",
    "\n",
    "#### 2.2 **Split Loss**\n",
    "\n",
    "Given a candidate split $\\theta$ applied at node $i$, the dataset $S_i$ is partitioned into a left subset $S_i^{\\text{left}}(\\theta)$ and a right subset $S_i^{\\text{right}}(\\theta)$:\n",
    "\n",
    "$$\n",
    "S_i^{\\text{left}}(\\theta) = \\{(x_j, y_j) \\in S_i \\mid x_{j, f} \\le t\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_i^{\\text{right}}(\\theta) = S_i \\setminus S_i^{\\text{left}}(\\theta),\n",
    "$$\n",
    "\n",
    "where $\\theta = (f, t)$ denotes the split feature index $f$ and the threshold value $t$.\n",
    "\n",
    "Let the number of samples in the left and right subsets be\n",
    "\n",
    "$$\n",
    "N_i^{\\text{left}} = |S_i^{\\text{left}}(\\theta)|, \\qquad \n",
    "N_i^{\\text{right}} = |S_i^{\\text{right}}(\\theta)|.\n",
    "$$\n",
    "\n",
    "Their corresponding class proportions are computed in the same way as in Section 2.1.\n",
    "\n",
    "\n",
    "#### 2.2.1 **Weighted Child Impurity**\n",
    "\n",
    "Given an impurity function $C(\\cdot)$ (e.g., Gini or entropy), the **split loss** at node $i$ for candidate split $\\theta$ is defined as the weighted sum of the left and right child impurities:\n",
    "    $$\n",
    "    L(S_i, \\theta) \n",
    "    = \n",
    "    \\frac{N_i^{\\text{left}}}{N_i} \n",
    "    \\, C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)\n",
    "    \\;+\\;\n",
    "    \\frac{N_i^{\\text{right}}}{N_i}\n",
    "    \\, C\\!\\left(S_i^{\\text{right}}(\\theta)\\right).\n",
    "    $$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)$ is the impurity (Gini or entropy) of the left child node.\n",
    "- $C\\!\\left(S_i^{\\text{right}}(\\theta)\\right)$ is the impurity of the right child node.\n",
    "\n",
    "\n",
    "#### 2.2.2 **Optimal Split Selection**\n",
    "\n",
    "The optimal split parameter is chosen by minimizing the split loss:\n",
    "\n",
    "$$\n",
    "\\theta^{*} = \\arg\\min_{\\theta} \\; L(S_i, \\theta).\n",
    "$$\n",
    "\n",
    "And this will be futher explained in the next part, Optimizer on how to actually implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea4014",
   "metadata": {},
   "source": [
    "## 3. Optimizer\n",
    "\n",
    "### 3.1 What is Optimized in CART\n",
    "\n",
    "CART performs a **greedy, recursive partitioning** - at each node, it selects the best split that maximizes information gain (or equivalently minimizes impurity).\n",
    "\n",
    "So the optimizer is essentially a **greedy search algorithm** that finds:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{(f,t)} \\; \\text{Impurity}(S_{\\text{left}}) + \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "where $f$ is the feature and $t$ is the threshold.\n",
    "\n",
    "#### 3.1.1 Objective Function\n",
    "\n",
    "CART minimizes an **impurity measure** (loss function) such as:\n",
    "- Gini Index:\n",
    "$$ G(S) = 1 - \\sum_{k=1}^{K}p_k^2 $$\n",
    "- Entropy:\n",
    "$$ H(S) = - \\sum_{k=1}^{K}p_klog(p_k)$$\n",
    "\n",
    "At each node:\n",
    "$$\n",
    "\\text{Gain}(S, f, t) = \\text{Impurity}(S) \n",
    "- \\frac{|S_{\\text{left}}|}{|S|} \\, \\text{Impurity}(S_{\\text{left}}) \n",
    "- \\frac{|S_{\\text{right}}|}{|S|} \\, \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "The algorithm chooses the feature $f*$ and threshold $t*$ that maximize this gain.\n",
    "#### 3.1.2 Pseudo-code\n",
    "\n",
    "Intuitively, the algorithm asks: “Which feature and cutoff most cleanly separates the classes?” By evaluating all possible splits and picking the one that reduces impurity the most, CART greedily chooses the single question that best organizes the data at this point in the tree. This local optimization step is repeated recursively to grow the whole decision tree.\n",
    "```python\n",
    "Inputs: dataset S, feature set F, impurity measure Impurity()\n",
    "\n",
    "best_gain ← 0  \n",
    "best_feature, best_threshold ← None  \n",
    "\n",
    "for each feature f in F:  \n",
    " for each possible threshold t in f:  \n",
    "  Split S into S_left and S_right using (f, t)  \n",
    "  if either split is empty: continue  \n",
    "  gain ← Impurity(S) \n",
    "     - (|S_left| / |S|) * Impurity(S_left)\n",
    "     - (|S_right| / |S|) * Impurity(S_right)  \n",
    "  if gain > best_gain:  \n",
    "   best_gain ← gain  \n",
    "   best_feature ← f  \n",
    "   best_threshold ← t  \n",
    "\n",
    "return (best_feature, best_threshold)\n",
    "\n",
    "\n",
    "This pseudocode describes how CART chooses the best split at a node by searching over all features and all possible thresholds. It begins by initializing best_gain and empty placeholders for the best split. For each feature, the algorithm tries every potential threshold and divides the dataset into **S_left** and **S_right**. If the split is invalid (one side is empty), it skips it. Otherwise, it computes the **impurity reduction** (gain): the impurity of the parent minus the weighted impurities of the two child subsets. If this gain is better than any previous one, the algorithm updates the best feature and threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff4cbc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de8de31-4d21-49ed-a953-18561f05383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def node_score_gini(probs):\n",
    "    '''\n",
    "    Compute Gini impurity for a probability vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs: 1D numpy array\n",
    "           Class probabilities p_k for a node, summing to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Gini impurity G = 1 - sum_k p_k^2.\n",
    "    '''\n",
    "    if probs.size == 0:\n",
    "        return 0.0\n",
    "    return 1.0 - np.sum(probs ** 2)\n",
    "\n",
    "\n",
    "def node_score_entropy(probs):\n",
    "    '''\n",
    "    Compute Entropy impurity for a probability vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs: 1D numpy array\n",
    "        Class probabilities p_k for a node, summing to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Entropy H = - sum_k p_k log(p_k) with the convention 0 * log 0 = 0.\n",
    "    '''\n",
    "    if probs.size == 0:\n",
    "        return 0.0\n",
    "    mask = probs > 0.0\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    p = probs[mask]\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "def _class_counts(y, n_classes):\n",
    "    '''\n",
    "    Count how many examples of each class appear in y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: 1D numpy array, shape (n_samples,)\n",
    "        Class labels for the samples in a node.\n",
    "    n_classes: int\n",
    "        Total number of distinct classes in the task.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts: 1D numpy array, shape (n_classes,)\n",
    "        counts[k] = number of samples of class k.\n",
    "    '''\n",
    "    return np.bincount(y, minlength=n_classes)\n",
    "\n",
    "\n",
    "def _to_probs(counts):\n",
    "    '''\n",
    "    Convert class counts to probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts: 1D numpy array\n",
    "        Class counts at a node.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    probs: 1D numpy array\n",
    "        Class probabilities p_k = counts[k] / sum(counts).\n",
    "        Returns all zeros if the node is empty.\n",
    "    '''\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return np.zeros_like(counts, dtype=float)\n",
    "    return counts.astype(float) / float(total)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper structure representing a single node in the CART tree.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    depth: int\n",
    "        Depth of the node (root has depth 0).\n",
    "    is_leaf: bool\n",
    "        Whether this node is a leaf.\n",
    "    feature_index: int or None\n",
    "        Index of feature used to split at this node (None for leaves).\n",
    "    threshold: float or None\n",
    "        Threshold value t for the split x_f <= t (None for leaves).\n",
    "    left: Node or None\n",
    "        Left child (samples with x_f <= t).\n",
    "    right: Node or None\n",
    "        Right child (samples with x_f > t).\n",
    "    class_counts : 1D numpy array\n",
    "        Counts of each class for samples reaching this node.\n",
    "    proba: 1D numpy array\n",
    "        Empirical class probability vector at this node.\n",
    "    prediction: int\n",
    "        Predicted class label at this node (argmax of 'proba').\n",
    "    n_samples: int\n",
    "        Number of samples that reached this node.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, depth, class_counts):\n",
    "        self.depth = depth\n",
    "        self.is_leaf = True    \n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        self.class_counts = class_counts.astype(int)\n",
    "        self.n_samples = int(class_counts.sum())\n",
    "\n",
    "        self.proba = _to_probs(self.class_counts)\n",
    "        self.prediction = int(np.argmax(self.proba))\n",
    "\n",
    "\n",
    "class DecisionTreeCART:\n",
    "    '''\n",
    "    CART (Classification and Regression Tree) classifier implemented from scratch.\n",
    "\n",
    "    Representation (project definition):\n",
    "\n",
    "        - Domain:   each sample X_i is an n-dimensional feature vector\n",
    "        - Labels:   Y = {0, 1, ..., K-1}\n",
    "        - Training data: D = {(x_i, y_i)}_{i=1}^N\n",
    "        - Output:  a binary decision tree of depth at most 'max_depth'\n",
    "                   Each leaf stores an empirical class probability vector,\n",
    "                   and predictions are argmax_k p_k at the leaf.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth: int or None\n",
    "        Maximum depth of the tree (root has depth 0). If None, the tree \n",
    "        can grow until all leaves are pure or no further split improves impurity.\n",
    "    min_samples_split: int\n",
    "        Minimum number of samples required at a node to consider splitting it.\n",
    "    impurity: str\n",
    "        Impurity measure to minimize at each split. Either 'gini' or 'entropy.'\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, impurity='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = max(min_samples_split, 2)\n",
    "        self.impurity_name = impurity\n",
    "        self.n_classes_ = None\n",
    "        self.n_features_ = None\n",
    "        self.root_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Train the CART classifier on labeled data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix.\n",
    "        y: array-like of shape (n_samples,)\n",
    "            Training labels in {0, 1, ..., K-1}.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: DecisionTreeCART\n",
    "            Fitted estimator.\n",
    "        '''\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y, dtype=int)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_features_ = n_features\n",
    "        self.n_classes_ = int(np.max(y)) + 1\n",
    "\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        self.root_ = self._build_tree(X, y, indices, depth=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict class labels for a matrix of input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: numpy array of shape (n_samples,)\n",
    "            Predicted class label for each sample.\n",
    "        '''\n",
    "        X = np.asarray(X)\n",
    "        preds = [self._predict_one(x, self.root_) for x in X]\n",
    "        return np.array(preds, dtype=int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Predict class probabilities for each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba: numpy array of shape (n_samples, n_classes)\n",
    "            probas[i, k] = estimated probability of class k for sample i.\n",
    "        '''\n",
    "        X = np.asarray(X)\n",
    "        probas = [self._predict_proba_one(x, self.root_) for x in X]\n",
    "        return np.vstack(probas)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        '''\n",
    "        Compute classification accuracy on a dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Fraction of correct predictions (between 0 and 1).\n",
    "        '''\n",
    "        y = np.asarray(y, dtype=int)\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        '''\n",
    "        Compute misclassification loss on a dataset.\n",
    "\n",
    "        Loss is defined as the fraction of incorrectly classified samples:\n",
    "\n",
    "            loss = 1 - accuracy\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "        y: array-like of shape (n_samples,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Misclassification rate (between 0 and 1).\n",
    "        '''\n",
    "        return 1.0 - self.accuracy(X, y)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Tree construction\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    def _build_tree(self, X, y, indices, depth):\n",
    "        '''\n",
    "        Recursively grow the tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array, shape (n_samples, n_features)\n",
    "            Full training feature matrix.\n",
    "        y: numpy array, shape (n_samples,)\n",
    "            Full training label vector.\n",
    "        indices: 1D numpy array\n",
    "            Indices of training samples that reach this node.\n",
    "        depth: int\n",
    "            Depth of the current node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        node : Node\n",
    "            The constructed (sub)tree root for this subset.\n",
    "        '''\n",
    "        y_node = y[indices]\n",
    "        counts = _class_counts(y_node, self.n_classes_)\n",
    "        node = Node(depth=depth, class_counts=counts)\n",
    "\n",
    "        # Stopping criteria\n",
    "        if self._is_terminal(node, indices, y_node):\n",
    "            return node\n",
    "\n",
    "        # Find best split using our optimizer\n",
    "        best_feature, best_threshold, best_gain = self._best_split(X, y_node, indices)\n",
    "\n",
    "        if best_feature is None or best_gain <= 0.0:\n",
    "            # No split improves impurity -> keep as leaf\n",
    "            return node\n",
    "\n",
    "        # Turn node into an internal split node\n",
    "        node.is_leaf = False\n",
    "        node.feature_index = best_feature\n",
    "        node.threshold = best_threshold\n",
    "\n",
    "        feature_values = X[indices, best_feature]\n",
    "        left_mask = feature_values <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_indices = indices[left_mask]\n",
    "        right_indices = indices[right_mask]\n",
    "\n",
    "        node.left = self._build_tree(X, y, left_indices, depth + 1)\n",
    "        node.right = self._build_tree(X, y, right_indices, depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _is_terminal(self, node, indices, y_node):\n",
    "        '''\n",
    "        Check whether a node should stop splitting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node: Node\n",
    "            Current node.\n",
    "        indices: 1D numpy array\n",
    "            Indices of samples reaching this node.\n",
    "        y_node: 1D numpy array\n",
    "            Labels of samples at this node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if the node should remain a leaf, False otherwise.\n",
    "\n",
    "        Stopping rules:\n",
    "\n",
    "            1. Node is empty (no samples).\n",
    "            2. All samples at the node share the same class label.\n",
    "            3. Node depth reached max_depth (if max_depth is set).\n",
    "            4. Number of samples is smaller than 'min_samples_split'.\n",
    "        '''\n",
    "        n_samples = indices.size\n",
    "\n",
    "        # 1. empty node\n",
    "        if n_samples == 0:\n",
    "            return True\n",
    "\n",
    "        # 2. pure node (all same label)\n",
    "        if np.unique(y_node).size == 1:\n",
    "            return True\n",
    "\n",
    "        # 3. max depth reached\n",
    "        if self.max_depth is not None and node.depth >= self.max_depth:\n",
    "            return True\n",
    "\n",
    "        # 4. not enough samples to split further\n",
    "        if n_samples < self.min_samples_split:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _best_split(self, X, y_node, indices):\n",
    "        '''\n",
    "        Find the best (feature, threshold) split for a node.\n",
    "\n",
    "        Follows the project pseudo-code:\n",
    "\n",
    "            best_gain <- 0\n",
    "            best_feature, best_threshold <- None\n",
    "\n",
    "            for each feature f in F:\n",
    "                for each possible threshold t in f:\n",
    "                    Split S into S_left and S_right using (f, t)\n",
    "                    if either split is empty: continue\n",
    "                    gain <- Impurity(S)\n",
    "                         - (|S_left| / |S|) * Impurity(S_left)\n",
    "                         - (|S_right| / |S|) * Impurity(S_right)\n",
    "                    if gain > best_gain:\n",
    "                        best_gain <- gain\n",
    "                        best_feature <- f\n",
    "                        best_threshold <- t\n",
    "\n",
    "            return (best_feature, best_threshold)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array, shape (n_samples, n_features)\n",
    "            Full feature matrix.\n",
    "        y_node: numpy array, shape (n_node_samples,)\n",
    "            Labels of samples reaching this node.\n",
    "        indices: 1D numpy array\n",
    "            Indices of samples forming the current dataset S.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        best_feature: int or None\n",
    "        best_threshold: float or None\n",
    "        best_gain: float\n",
    "        '''\n",
    "        n_node_samples = indices.size\n",
    "        if n_node_samples == 0:\n",
    "            return None, None, 0.0\n",
    "\n",
    "        # Parent impurity\n",
    "        parent_counts = _class_counts(y_node, self.n_classes_)\n",
    "        parent_probs = _to_probs(parent_counts)\n",
    "        parent_impurity = self._impurity(parent_probs)\n",
    "\n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        # Iterate over all features f in F\n",
    "        for f in range(self.n_features_):\n",
    "            feature_values = X[indices, f]\n",
    "\n",
    "            unique_vals = np.unique(feature_values)\n",
    "            if unique_vals.size <= 1:\n",
    "                # No meaningful split on this feature\n",
    "                continue\n",
    "\n",
    "            # Candidate thresholds: midpoints between unique sorted values\n",
    "            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n",
    "\n",
    "            for t in thresholds:\n",
    "                left_mask = feature_values <= t\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if not np.any(left_mask) or not np.any(right_mask):\n",
    "                    continue\n",
    "\n",
    "                left_indices = indices[left_mask]\n",
    "                right_indices = indices[right_mask]\n",
    "\n",
    "                y_left = y_node[left_mask]\n",
    "                y_right = y_node[right_mask]\n",
    "\n",
    "                left_counts = _class_counts(y_left, self.n_classes_)\n",
    "                right_counts = _class_counts(y_right, self.n_classes_)\n",
    "\n",
    "                left_probs = _to_probs(left_counts)\n",
    "                right_probs = _to_probs(right_counts)\n",
    "\n",
    "                impur_left = self._impurity(left_probs)\n",
    "                impur_right = self._impurity(right_probs)\n",
    "\n",
    "                w_left = float(left_indices.size) / float(n_node_samples)\n",
    "                w_right = float(right_indices.size) / float(n_node_samples)\n",
    "\n",
    "                gain = parent_impurity - w_left * impur_left - w_right * impur_right\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = f\n",
    "                    best_threshold = t\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _impurity(self, probs):\n",
    "        '''\n",
    "        Dispatch to the chosen impurity function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probs: 1D numpy array\n",
    "            Class probability vector for a node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Impurity value according to self.impurity_name.\n",
    "        '''\n",
    "        if self.impurity_name == 'gini':\n",
    "            return node_score_gini(probs)\n",
    "        else:\n",
    "            return node_score_entropy(probs)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        '''\n",
    "        Predict class label for a single sample by traversing the tree.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:1D numpy array of shape (n_features,)\n",
    "            Feature vector for one sample.\n",
    "        node: Node\n",
    "            Current node in the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Predicted class label.\n",
    "        '''\n",
    "        while not node.is_leaf:\n",
    "            f = node.feature_index\n",
    "            t = node.threshold\n",
    "            if x[f] <= t:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction\n",
    "\n",
    "    def _predict_proba_one(self, x, node):\n",
    "        '''\n",
    "        Predict class probabilities for a single sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: 1D numpy array of shape (n_features,)\n",
    "            Feature vector for one sample.\n",
    "        node:Node\n",
    "            Current node in the tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs: 1D numpy array of shape (n_classes,)\n",
    "            Empirical class probabilities stored at the leaf.\n",
    "        '''\n",
    "        while not node.is_leaf:\n",
    "            f = node.feature_index\n",
    "            t = node.threshold\n",
    "            if x[f] <= t:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1094318",
   "metadata": {},
   "source": [
    "# Check Model\n",
    "\n",
    "In this section, we design unit tests for our `DecisionTreeCART` implementation and compare it against `sklearn.tree.DecisionTreeClassifier` on a public dataset (the breast cancer dataset). The goals are:\n",
    "- verify that each method of our class works correctly in isolation,\n",
    "- check that edge cases are handled properly,\n",
    "- and demonstrate that our implementation can successfully reproduce sklearn’s CART results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c4830a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (398, 30)\n",
      "X_test shape: (171, 30)\n",
      "Class counts: [212 357]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pytest\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Data setup: breast cancer dataset\n",
    "# ---------------------------------------------------\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target  # binary labels {0, 1}\n",
    "\n",
    "# train / test split (fixed random_state for reproducibility)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=0,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Class counts:\", np.bincount(y))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Helper to create our CART model\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def make_cart_model(max_depth=5, min_samples_split=2, impurity='gini'):\n",
    "    \"\"\"\n",
    "    Helper function to initialize our CART model with consistent hyperparameters.\n",
    "    \"\"\"\n",
    "    model = DecisionTreeCART(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        impurity=impurity,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad69b75",
   "metadata": {},
   "source": [
    "### Test 1–3: Basic functionality\n",
    "\n",
    "- **Test 1 – fit():** check that training runs without error.  \n",
    "- **Test 2 – predict():** check output shape, label range, and report train/test accuracy.  \n",
    "- **Test 3 – loss():** check that `loss` returns a finite scalar (misclassification error in [0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54ce17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 passed: train() runs without error.\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "model = make_cart_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test 1 passed: train() runs without error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b8d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 passed: predict() shape & value checks passed. Train accuracy = 0.997\n",
      "Test accuracy = 0.918\n"
     ]
    }
   ],
   "source": [
    "# Test 2: predict() should produce outputs with correct shape and valid class values\n",
    "\n",
    "model = make_cart_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Check output shape\n",
    "assert y_pred_train.shape == y_train.shape, \\\n",
    "    f\"Prediction shape mismatch: y_pred shape={y_pred_train.shape}, y_train shape={y_train.shape}\"\n",
    "\n",
    "# Check value range (breast_cancer is a binary classification dataset)\n",
    "unique_vals = np.unique(y_pred_train)\n",
    "assert set(unique_vals).issubset({0, 1}), \\\n",
    "    f\"Predicted values must be 0/1. Found values: {unique_vals}\"\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Test 2 passed: predict() shape & value checks passed. Train accuracy = {train_acc:.3f}\")\n",
    "\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "print(f\"Test accuracy = {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a06194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 passed: loss() returns a valid finite scalar. Train loss = 0.002513\n"
     ]
    }
   ],
   "source": [
    "# Test 3: loss() should return a finite scalar value\n",
    "\n",
    "model = make_cart_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_loss = model.loss(X_train, y_train)\n",
    "\n",
    "assert np.isscalar(train_loss), \"loss() should return a scalar value.\"\n",
    "assert np.isfinite(train_loss), \"loss() should not return NaN or infinity.\"\n",
    "\n",
    "print(f\"Test 3 passed: loss() returns a valid finite scalar. Train loss = {train_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747a13e",
   "metadata": {},
   "source": [
    "Our loss function is defined as the misclassification error rate, therefore it should be a scalar between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64412045",
   "metadata": {},
   "source": [
    "### Test 4: Edge cases and method-level tests\n",
    "\n",
    "We use small toy datasets to verify that our CART implementation behaves correctly under extreme scenarios and that core methods work as intended.\n",
    "\n",
    "**Edge cases**\n",
    "- **Test 4.1 – All labels identical (only one class)**\n",
    "- **Test 4.2 – Single feature only**\n",
    "- **Test 4.3 – All-zero features**\n",
    "\n",
    "**Method-level tests**\n",
    "- **Test 4.4 – `predict_proba()`**: correct shape, valid probability distribution, and consistency with `predict`\n",
    "- **Test 4.5 – `accuracy()`**: matches manual computation on a tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d5eb1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_toy:\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "y_toy: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# A small toy dataset for edge case testing\n",
    "X_toy = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "])\n",
    "y_toy = np.array([0, 0, 1, 1])\n",
    "\n",
    "print(\"X_toy:\\n\", X_toy)\n",
    "print(\"y_toy:\", y_toy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cb7275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.1 passed: all-zero labels edge case handled correctly.\n",
      "Predicted labels: [0 0 0 0]\n",
      "Loss on all-zero labels: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test 4.1: all labels are zero (only one class present)\n",
    "\n",
    "model_zero = make_cart_model()\n",
    "\n",
    "y_all_zero = np.zeros_like(y_toy)\n",
    "model_zero.fit(X_toy, y_all_zero)\n",
    "\n",
    "y_pred_zero = model_zero.predict(X_toy)\n",
    "loss_zero = model_zero.loss(X_toy, y_all_zero)\n",
    "\n",
    "assert y_pred_zero.shape == y_all_zero.shape\n",
    "assert np.isfinite(loss_zero)\n",
    "\n",
    "print(\"Test 4.1 passed: all-zero labels edge case handled correctly.\")\n",
    "print(\"Predicted labels:\", y_pred_zero)\n",
    "print(\"Loss on all-zero labels:\", loss_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdeffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.2 passed: single-feature edge case handled correctly.\n",
      "Predicted labels: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test 4.2: dataset contains only one feature\n",
    "\n",
    "model_single = make_cart_model()\n",
    "\n",
    "X_single = X_toy[:, :1]  # Use only the first feature\n",
    "model_single.fit(X_single, y_toy)\n",
    "\n",
    "y_pred_single = model_single.predict(X_single)\n",
    "assert y_pred_single.shape == y_toy.shape\n",
    "\n",
    "print(\"Test 4.2 passed: single-feature edge case handled correctly.\")\n",
    "\n",
    "assert np.array_equal(y_pred_single, y_toy)\n",
    "print(\"Predicted labels:\", y_pred_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f302c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.3 passed: all-zero features edge case handled correctly.\n",
      "Predicted labels: [0 0 0 0]\n",
      "Loss on all-zero features: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Test 4.3: all feature values are zero\n",
    "\n",
    "model_feat_zero = make_cart_model()\n",
    "\n",
    "X_zeros = np.zeros_like(X_toy)\n",
    "model_feat_zero.fit(X_zeros, y_toy)\n",
    "\n",
    "y_pred_zeros = model_feat_zero.predict(X_zeros)\n",
    "loss_zeros = model_feat_zero.loss(X_zeros, y_toy)\n",
    "\n",
    "assert y_pred_zeros.shape == y_toy.shape\n",
    "assert np.isfinite(loss_zeros)\n",
    "\n",
    "print(\"Test 4.3 passed: all-zero features edge case handled correctly.\")\n",
    "print(\"Predicted labels:\", y_pred_zeros)\n",
    "print(\"Loss on all-zero features:\", loss_zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6fd4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4b passed: predict_proba has correct shape, rows sum to 1, and argmax matches predict().\n"
     ]
    }
   ],
   "source": [
    "# Test 4.4: predict_proba() shape and probabilities\n",
    "\n",
    "model = make_cart_model(max_depth=5, min_samples_split=2, impurity='gini')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "proba = model.predict_proba(X_test)\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "assert proba.shape == (X_test.shape[0], n_classes), \\\n",
    "    f\"predict_proba shape {proba.shape} does not match (n_samples, n_classes).\"\n",
    "\n",
    "row_sums = proba.sum(axis=1)\n",
    "assert np.allclose(row_sums, 1.0, atol=1e-12), \\\n",
    "    \"Each row of predict_proba should sum to 1.\"\n",
    "\n",
    "y_pred_from_proba = np.argmax(proba, axis=1)\n",
    "y_pred = model.predict(X_test)\n",
    "assert np.array_equal(y_pred_from_proba, y_pred), \\\n",
    "    \"argmax over predict_proba should match predict().\"\n",
    "\n",
    "print(\"Test 4b passed: predict_proba has correct shape, rows sum to 1, and argmax matches predict().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ecf3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4c passed: model.accuracy matches manual accuracy on the toy dataset.\n"
     ]
    }
   ],
   "source": [
    "# Test 4.5: accuracy() matches manual computation\n",
    "X_toy_small = np.array([[0], [1], [2], [3]])\n",
    "y_toy_small = np.array([0, 0, 1, 1])\n",
    "\n",
    "model = make_cart_model(max_depth=2, min_samples_split=2, impurity='gini')\n",
    "model.fit(X_toy_small, y_toy_small)\n",
    "\n",
    "y_pred_toy = model.predict(X_toy_small)\n",
    "manual_acc = np.mean(y_pred_toy == y_toy_small)\n",
    "model_acc = model.accuracy(X_toy_small, y_toy_small)\n",
    "\n",
    "assert np.isclose(manual_acc, model_acc), \\\n",
    "    f\"Manual accuracy {manual_acc} does not match model.accuracy {model_acc}.\"\n",
    "\n",
    "print(\"Test 4c passed: model.accuracy matches manual accuracy on the toy dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da70f4",
   "metadata": {},
   "source": [
    "### (Updated--more strict thresholds & add the case when impurity='entropy'& goal description)\n",
    "### Test 5: Comparison with sklearn on a public dataset\n",
    "\n",
    "We now compare our `DecisionTreeCART` implementation to `sklearn.tree.DecisionTreeClassifier` on the breast cancer dataset.  \n",
    "We use the same hyperparameters (max_depth, min_samples_split, impurity criterion) and fix `random_state=3` for sklearn to remove randomness in tie-breaking.  \n",
    "\n",
    "We perform two comparisons:\n",
    "- **Test 5.1 – Gini impurity**\n",
    "- **Test 5.2 – Entropy impurity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78754ed4-50c7-45c4-8c0f-e10073b37df5",
   "metadata": {},
   "source": [
    "**（To be discussed/checked）其实我是set_random_seed=3的时候才会完全一样，不知道这个会不会是一个concern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7f5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn CART test accuracy: 0.918129\n",
      "Our CART test accuracy: 0.918129\n",
      "Same predictions as sklearn? False\n",
      "Absolute accuracy difference: 0.00000000\n",
      "Test 5 passed: our CART implementation matches sklearn CART (using 'gini' impurity).\n",
      "\n",
      "Sklearn CART test accuracy: 0.929825\n",
      "Our CART test accuracy: 0.929825\n",
      "Same predictions as sklearn? True\n",
      "Absolute accuracy difference: 0.00000000\n",
      "Test 5 passed: our CART implementation matches sklearn CART (using 'entropy' impurity).\n"
     ]
    }
   ],
   "source": [
    "# Test 5.1 Compare our predictions with sklearn's CART classifier\n",
    "# Sklearn CART (using Gini impurity)\n",
    "# Goal of this test:\n",
    "#   This test evaluates whether our CART implementation can\n",
    "#   successfully reproduce the behavior of sklearn’s\n",
    "#   DecisionTreeClassifier when trained on the same dataset\n",
    "#   with identical hyperparameters.\n",
    "\n",
    "sk_cart = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "sk_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = sk_cart.predict(X_test)\n",
    "sk_acc = accuracy_score(y_test, y_pred_sk)\n",
    "\n",
    "print(f\"Sklearn CART test accuracy: {sk_acc:.6f}\")\n",
    "\n",
    "# Our own CART implementation\n",
    "my_cart = make_cart_model()\n",
    "my_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_my = my_cart.predict(X_test)\n",
    "my_acc = accuracy_score(y_test, y_pred_my)\n",
    "\n",
    "print(f\"Our CART test accuracy: {my_acc:.6f}\")\n",
    "\n",
    "\n",
    "same_predictions = np.array_equal(y_pred_sk, y_pred_my)\n",
    "acc_diff = abs(sk_acc - my_acc)\n",
    "\n",
    "print(\"Same predictions as sklearn?\", same_predictions)\n",
    "print(f\"Absolute accuracy difference: {acc_diff:.8f}\")\n",
    "\n",
    "# Depending on implementation details, exact matching or near-matching are acceptable.\n",
    "assert my_acc == pytest.approx(sk_acc, abs=1e-12), \\\n",
    "    \"Our CART accuracy should be extremely close to sklearn's CART accuracy.\"\n",
    "\n",
    "print(\"Test 5 passed: our CART implementation matches sklearn CART (using 'gini' impurity).\\n\")\n",
    "\n",
    "#===============================================================================================================\n",
    "\n",
    "# Test 5.2 Compare our predictions with sklearn's CART classifier\n",
    "# Sklearn CART (using entropy impurity)\n",
    "\n",
    "sk_cart = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "sk_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = sk_cart.predict(X_test)\n",
    "sk_acc = accuracy_score(y_test, y_pred_sk)\n",
    "\n",
    "print(f\"Sklearn CART test accuracy: {sk_acc:.6f}\")\n",
    "\n",
    "# Our own CART implementation\n",
    "my_cart = make_cart_model(impurity='entropy')\n",
    "my_cart.fit(X_train, y_train)\n",
    "\n",
    "y_pred_my = my_cart.predict(X_test)\n",
    "my_acc = accuracy_score(y_test, y_pred_my)\n",
    "\n",
    "print(f\"Our CART test accuracy: {my_acc:.6f}\")\n",
    "\n",
    "\n",
    "same_predictions = np.array_equal(y_pred_sk, y_pred_my)\n",
    "acc_diff = abs(sk_acc - my_acc)\n",
    "\n",
    "print(\"Same predictions as sklearn?\", same_predictions)\n",
    "print(f\"Absolute accuracy difference: {acc_diff:.8f}\")\n",
    "\n",
    "# Depending on implementation details, exact matching or near-matching are acceptable.\n",
    "assert my_acc == pytest.approx(sk_acc, abs=1e-12), \\\n",
    "    \"Our CART accuracy should be extremely close to sklearn's CART accuracy.\"\n",
    "\n",
    "print(\"Test 5 passed: our CART implementation matches sklearn CART (using 'entropy' impurity).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021a9b7-685b-4b7d-bcb8-90746cfd3e7b",
   "metadata": {},
   "source": [
    "### Test 6: Node impurity calculation\n",
    "\n",
    "Finally, we directly unit-test our impurity functions `node_score_gini` and `node_score_entropy` against sklearn’s impurity values on several label distributions:\n",
    "\n",
    "- pure node (all labels identical),\n",
    "- balanced 50/50 node,\n",
    "- skewed binary labels,\n",
    "- multi-class labels.\n",
    "\n",
    "We construct a root-only sklearn tree and compare the impurity stored at the root to our implementation (up to a log-base factor for entropy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47215a2e-513b-4234-9e15-bcc9097c649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impurity tests passed: our impurity calculations match sklearn exactly (Gini) and up to log-base (Entropy).\n"
     ]
    }
   ],
   "source": [
    "# Tests 6: Our CART impurity vs. sklearn impurity\n",
    "# Goal of this block:\n",
    "#   For several different label distributions, compare our\n",
    "#   node_score_gini and node_score_entropy against sklearn's\n",
    "#   impurity values at a root-only tree. This directly unit-tests\n",
    "#   our impurity functions and checks that they match sklearn\n",
    "#   (up to log-base for entropy).\n",
    "\n",
    "\n",
    "def sklearn_impurity(y, criterion):\n",
    "    '''\n",
    "    Compute sklearn impurity for labels y.\n",
    "    We force a ROOT-ONLY tree (no splits) by:\n",
    "      - Using a dummy constant feature X_dummy\n",
    "      - Setting min_samples_split > n_samples to prevent splitting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: array-like labels for the samples.\n",
    "    criterion : {'gini', 'entropy'}\n",
    "        Impurity measure to use internally in sklearn.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Impurity value stored at the root node.\n",
    "    '''\n",
    "    y = np.asarray(y)\n",
    "    X_dummy = np.zeros((len(y), 1))  \n",
    "\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=1,       \n",
    "        min_samples_split=len(y) + 1, \n",
    "        random_state=0\n",
    "    )\n",
    "    clf.fit(X_dummy, y)\n",
    "\n",
    "    return clf.tree_.impurity[0]\n",
    "    \n",
    "#===============================================================================================================\n",
    "# Test 6.1: Pure node (all labels identical)\n",
    "# Edge case: impurity should be zero for both gini and entropy.\n",
    "\n",
    "y_pure = [0, 0, 0, 0]\n",
    "sk_gini = sklearn_impurity(y_pure, 'gini')\n",
    "sk_entropy = sklearn_impurity(y_pure, 'entropy')\n",
    "\n",
    "n_classes = len(np.unique(y_pure))\n",
    "counts = _class_counts(np.array(y_pure), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "# Convert sklearn's base-2 entropy to natural log (based on our definition from the reference)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "#===============================================================================================================\n",
    "# Test 6.2: Balanced labels (50/50)\n",
    "# Edge case: maximal impurity for 2 classes (gini=0.5, entropy=1 bit).\n",
    "\n",
    "y_bal = [0, 1]\n",
    "sk_gini = sklearn_impurity(y_bal, 'gini')\n",
    "sk_entropy = sklearn_impurity(y_bal, 'entropy')\n",
    "\n",
    "n_classes = len(np.unique(y_bal))\n",
    "counts = _class_counts(np.array(y_bal), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "#===============================================================================================================\n",
    "# Test 6.3: Skewed labels (e.g., 2 zeros, 3 ones)\n",
    "# Regular case: impurity should lie strictly between 0 and the balanced-case maximum.\n",
    "\n",
    "y_skew = [0, 0, 1, 1, 1]\n",
    "sk_gini = sklearn_impurity(y_skew, 'gini')\n",
    "sk_entropy = sklearn_impurity(y_skew, 'entropy')\n",
    "\n",
    "n_classes = len(np.unique(y_skew))\n",
    "counts = _class_counts(np.array(y_skew), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "#===============================================================================================================\n",
    "# Test 6.4: Three-class labels\n",
    "# Multi-class case: checks that our impurity generalizes beyond binary labels.\n",
    "\n",
    "y_three = [0, 1, 2, 2, 2, 1]\n",
    "sk_gini = sklearn_impurity(y_three, \"gini\")\n",
    "sk_entropy = sklearn_impurity(y_three, \"entropy\")\n",
    "\n",
    "n_classes = len(np.unique(y_three))\n",
    "counts = _class_counts(np.array(y_three), n_classes)\n",
    "probs   = _to_probs(counts)\n",
    "\n",
    "assert node_score_gini(probs) == pytest.approx(sk_gini, abs=1e-12)\n",
    "assert node_score_entropy(probs) == pytest.approx(sk_entropy * np.log(2), abs=1e-12)\n",
    "\n",
    "print(\"Impurity tests passed: our impurity calculations match sklearn exactly (Gini) and up to log-base (Entropy).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58761419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# data = np.genfromtxt(\"breast_cancer.csv\", delimiter=\",\", dtype=str, skip_header=1)\n",
    "# diagnosis = data[:, 1]\n",
    "# X = data[:, 2:].astype(float)\n",
    "# y = (diagnosis == \"M\").astype(int)\n",
    "\n",
    "# print(\"X shape:\", X.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faadab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 1.0\n",
      "val acc: 0.9298245614035088\n",
      "test acc: 0.8245614035087719\n",
      "--- CART TREE ---\n",
      "[Feature 22 <= 105.1500] gain=0.3611\n",
      "\n",
      "  [Feature 24 <= 0.1759] gain=0.0592\n",
      "\n",
      "    [Feature 0 <= 14.9800] gain=0.0106\n",
      "\n",
      "      [Feature 27 <= 0.1807] gain=0.0108\n",
      "\n",
      "        [Feature 20 <= 15.7250] gain=0.0043\n",
      "\n",
      "          [Feature 12 <= 4.1055] gain=0.0058\n",
      "\n",
      "            [Feature 21 <= 33.1050] gain=0.0030\n",
      "\n",
      "              Leaf(label=1, samples=?)\n",
      "              [Feature 0 <= 12.0450] gain=0.3750\n",
      "\n",
      "                Leaf(label=1, samples=?)\n",
      "                Leaf(label=0, samples=?)\n",
      "            [Feature 0 <= 12.2650] gain=0.5000\n",
      "\n",
      "              Leaf(label=0, samples=?)\n",
      "              Leaf(label=1, samples=?)\n",
      "          [Feature 8 <= 0.1782] gain=0.3457\n",
      "\n",
      "            Leaf(label=1, samples=?)\n",
      "            Leaf(label=0, samples=?)\n",
      "        Leaf(label=0, samples=?)\n",
      "      Leaf(label=0, samples=?)\n",
      "    Leaf(label=0, samples=?)\n",
      "  [Feature 22 <= 114.4500] gain=0.0418\n",
      "\n",
      "    [Feature 1 <= 19.7100] gain=0.3174\n",
      "\n",
      "      [Feature 0 <= 14.1100] gain=0.3200\n",
      "\n",
      "        Leaf(label=0, samples=?)\n",
      "        Leaf(label=1, samples=?)\n",
      "      Leaf(label=0, samples=?)\n",
      "    [Feature 7 <= 0.0280] gain=0.0159\n",
      "\n",
      "      Leaf(label=1, samples=?)\n",
      "      Leaf(label=0, samples=?)\n",
      "--- END ---\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# random.seed(0)\n",
    "# n = len(y)\n",
    "# split1 = int(0.6 * n)\n",
    "# split2 = int(0.8 * n)\n",
    "\n",
    "# X_train, y_train = X[:split1], y[:split1]\n",
    "# X_valid, y_valid = X[split1:split2], y[split1:split2]\n",
    "# X_test, y_test = X[split2:], y[split2:]\n",
    "\n",
    "\n",
    "# from src.cart import DecisionTreeCART\n",
    "# tree = DecisionTreeCART(max_depth=40, min_samples_split=2)\n",
    "# tree.fit(X_train, y_train)\n",
    "\n",
    "# print(\"train acc:\", tree.accuracy(X_train, y_train))\n",
    "# print(\"val acc:\", tree.accuracy(X_valid, y_valid))\n",
    "# print(\"test acc:\", tree.accuracy(X_test, y_test))\n",
    "\n",
    "# tree.print_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c7d8b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "scikit-learn developers (2024) *Decision Trees: Mathematical Formulation*. Available at: https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac43de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
