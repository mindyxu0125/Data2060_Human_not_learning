{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7917ab9",
   "metadata": {},
   "source": [
    "## DATA2060 Final Project\n",
    "\n",
    "Model: **CART for classification**\n",
    "\n",
    "Team Members:\n",
    "- Muxin Fu\n",
    "- Yixiao Zhang\n",
    "- Jingming Xu\n",
    "- Mingrui Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897fd31",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14328149",
   "metadata": {},
   "source": [
    "#### 0.1 Overview  \n",
    "The Classification and Regression Tree (CART) algorithm is a nonparametric supervised learning method that builds a binary decision tree for classification tasks. At each step, the algorithm selects a feature and threshold that create two child nodes with lower class impurity, using criteria such as Gini impurity or entropy. Through this recursive partitioning, CART represents the classifier as a set of piecewise-constant regions, where each leaf corresponds to a predicted class label. Because the sequence of splits directly mirrors the decision-making process, CART offers a transparent and intuitive model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf3270",
   "metadata": {},
   "source": [
    "#### 0.2 Advantages  \n",
    "CART offers several notable strengths that contribute to its widespread use as a baseline classifier.  \n",
    "First, the model is highly interpretable: each internal node corresponds to a clear “if–then” condition based on a single feature, allowing the entire decision path to be easily traced and communicated. This transparency is particularly valuable in settings where model explanations are required.  \n",
    "\n",
    "Second, CART is able to capture nonlinear relationships and feature interactions without relying on explicit transformations or parametric assumptions. Its recursive splitting procedure enables the model to adapt flexibly to irregular or complex decision boundaries, providing expressive power beyond that of linear models.  \n",
    "\n",
    "Moreover, CART requires minimal preprocessing. It can accommodate both numerical and categorical variables, is robust to monotonic feature scaling, and implicitly performs feature selection by choosing splits only on informative variables. These characteristics make CART convenient to implement and reliable across a wide range of practical applications.\n",
    "\n",
    "#### 0.3 Disadvantages  \n",
    "Despite its advantages, CART also presents several limitations that must be considered.  \n",
    "Most importantly, the model is prone to overfitting when allowed to grow without constraints. As emphasized in the bias–complexity trade-off discussed in the course reading, increasing model flexibility reduces approximation error but raises estimation error, causing deep, unpruned trees to exhibit high variance and poor generalization.  \n",
    "\n",
    "CART also tends to be unstable: small perturbations in the training data can alter early splits, resulting in substantially different tree structures. This sensitivity undermines the model’s reliability, especially in contexts requiring stable predictions.  \n",
    "\n",
    "Finally, because CART relies exclusively on axis-aligned splits, it may need many successive partitions to approximate diagonal or curved decision boundaries, leading to unnecessarily deep and complex trees. These shortcomings motivate the use of pruning techniques and more advanced ensemble methods, such as Random Forests and Gradient Boosting, which address variance and stability issues more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81ff2",
   "metadata": {},
   "source": [
    "### 1. Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc5ddd",
   "metadata": {},
   "source": [
    "### 2. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab2653",
   "metadata": {},
   "source": [
    "In the classification setting, losses are the **mearuses of impurity**.  CART minimizes impruity and the loss is defined per split. Generally speaking, **Gini** and **Entropy** are good measures.\n",
    "\n",
    "To compute Loss, we need: \n",
    "* Impurity measure, \n",
    "* Split loss based on choosen impurity measure.\n",
    "\n",
    "In the scikit-learn, this is determined by the parameter **criterion**: *{“gini”, “entropy”, “log_loss”}, default=”gini”* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e29c6",
   "metadata": {},
   "source": [
    "#### 2.1 **Impurity Function**\n",
    "\n",
    "For a $K$-class classification problem, consider node $i$ containing a subset of samples\n",
    "\n",
    "$$S_i = \\{(x_j, y_j)\\}_{j \\in \\mathcal{I}_i}, \\qquad N_i = |S_i|.$$\n",
    "\n",
    "The number of samples in node $i$ that belong to class $k$ is\n",
    "\n",
    "$$n_{i,k} = \\sum_{j \\in \\mathcal{I}_i} \\mathbf{1}(y_j = k).$$\n",
    "\n",
    "The class proportion of class $k$ in node $i$ is\n",
    "\n",
    "$$p_{i,k} = \\frac{n_{i,k}}{N_i}, \\qquad k = 1, \\dots, K.$$\n",
    "$$\\sum_{k=1}^K p_{i,k} = 1,\\text{and  } p_{i,k} \\ge 0 \\quad \\text{for } k = 1, \\dots, K.$$\n",
    "\n",
    "##### 2.1.1 **Gini**\n",
    "\n",
    "\n",
    "- The Gini impurity of node $i$ is:   \n",
    "$$G_i = 1 - \\sum_{k=1}^K p_{i,k}^2.$$\n",
    "\n",
    "##### 2.1.2 **Entropy**\n",
    "\n",
    "- The entropy impurity of node $i$ is\n",
    "$$H_i = - \\sum_{k=1}^K p_{i,k} \\log p_{i,k},$$\n",
    "\n",
    "- And we assume $0 \\log 0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad0f55",
   "metadata": {},
   "source": [
    "#### 2.2 **Split Loss**\n",
    "\n",
    "Given a candidate split $\\theta$ applied at node $i$, the dataset $S_i$ is partitioned into a left subset $S_i^{\\text{left}}(\\theta)$ and a right subset $S_i^{\\text{right}}(\\theta)$:\n",
    "\n",
    "$$\n",
    "S_i^{\\text{left}}(\\theta) = \\{(x_j, y_j) \\in S_i \\mid x_{j, f} \\le t\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_i^{\\text{right}}(\\theta) = S_i \\setminus S_i^{\\text{left}}(\\theta),\n",
    "$$\n",
    "\n",
    "where $\\theta = (f, t)$ denotes the split feature index $f$ and the threshold value $t$.\n",
    "\n",
    "Let the number of samples in the left and right subsets be\n",
    "\n",
    "$$\n",
    "N_i^{\\text{left}} = |S_i^{\\text{left}}(\\theta)|, \\qquad \n",
    "N_i^{\\text{right}} = |S_i^{\\text{right}}(\\theta)|.\n",
    "$$\n",
    "\n",
    "Their corresponding class proportions are computed in the same way as in Section 2.1.\n",
    "\n",
    "\n",
    "##### 2.2.1 **Weighted Child Impurity**\n",
    "\n",
    "Given an impurity function $C(\\cdot)$ (e.g., Gini or entropy), the **split loss** at node $i$ for candidate split $\\theta$ is defined as the weighted sum of the left and right child impurities:\n",
    "\n",
    "$$\n",
    "L(S_i, \\theta) \n",
    "= \n",
    "\\frac{N_i^{\\text{left}}}{N_i} \n",
    "\\, C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)\n",
    "\\;+\\;\n",
    "\\frac{N_i^{\\text{right}}}{N_i}\n",
    "\\, C\\!\\left(S_i^{\\text{right}}(\\theta)\\right).\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)$ is the impurity (Gini or entropy) of the left child node.\n",
    "- $C\\!\\left(S_i^{\\text{right}}(\\theta)\\right)$ is the impurity of the right child node.\n",
    "\n",
    "\n",
    "##### 2.2.2 **Optimal Split Selection**\n",
    "\n",
    "The optimal split parameter is chosen by minimizing the split loss:\n",
    "\n",
    "$$\n",
    "\\theta^{*} = \\arg\\min_{\\theta} \\; L(S_i, \\theta).\n",
    "$$\n",
    "\n",
    "And this will be futher explained in the next part, Optimizer on how to actually implement it.\n",
    "\n",
    "- ***Reference***: scikit-learn mathematical formulation https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eed380",
   "metadata": {},
   "source": [
    "### 3. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71bbe77",
   "metadata": {},
   "source": [
    "### 3.1 What is Optimized in CART\n",
    "\n",
    "CART performs a **greedy, recursive partitioning** - at each node, it selects the best split that maximizes information gain (or equivalently minimizes impurity).\n",
    "\n",
    "So the optimizer is essentially a **greedy search algorithm** that finds:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{(f,t)} \\; \\text{Impurity}(S_{\\text{left}}) + \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "where $f$ is the feature and $t$ is the threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d074bd",
   "metadata": {},
   "source": [
    "#### 3.1.1 Objective Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c847f",
   "metadata": {},
   "source": [
    "CART minimizes an **impurity measure** (loss function) such as:\n",
    "- Gini Index:\n",
    "$$ G(S) = 1 - \\sum_{k=1}^{K}p_k^2 $$\n",
    "- Entropy:\n",
    "$$ H(S) = - \\sum_{k=1}^{K}p_klog(p_k)$$\n",
    "\n",
    "At each node:\n",
    "$$\n",
    "\\text{Gain}(S, f, t) = \\text{Impurity}(S) \n",
    "- \\frac{|S_{\\text{left}}|}{|S|} \\, \\text{Impurity}(S_{\\text{left}}) \n",
    "- \\frac{|S_{\\text{right}}|}{|S|} \\, \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "The algorithm chooses the feature $f*$ and threshold $t*$ that maximize this gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c31b7",
   "metadata": {},
   "source": [
    "#### 3.1.2 Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0b67a",
   "metadata": {},
   "source": [
    "```python\n",
    "Inputs: dataset S, feature set F, impurity measure Impurity()\n",
    "\n",
    "best_gain ← 0  \n",
    "best_feature, best_threshold ← None  \n",
    "\n",
    "for each feature f in F:  \n",
    " for each possible threshold t in f:  \n",
    "  Split S into S_left and S_right using (f, t)  \n",
    "  if either split is empty: continue  \n",
    "  gain ← Impurity(S) \n",
    "     - (|S_left| / |S|) * Impurity(S_left)\n",
    "     - (|S_right| / |S|) * Impurity(S_right)  \n",
    "  if gain > best_gain:  \n",
    "   best_gain ← gain  \n",
    "   best_feature ← f  \n",
    "   best_threshold ← t  \n",
    "\n",
    "return (best_feature, best_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c447cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
