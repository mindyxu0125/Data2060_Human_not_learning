{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7917ab9",
   "metadata": {},
   "source": [
    "## DATA2060 Final Project\n",
    "\n",
    "Model: **CART for classification**\n",
    "\n",
    "Team Members:\n",
    "- Muxin Fu\n",
    "- Yixiao Zhang\n",
    "- Jingmin Xu\n",
    "- Mingrui Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897fd31",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14328149",
   "metadata": {},
   "source": [
    "#### 0.1 Overview  \n",
    "The Classification and Regression Tree (CART) algorithm is a nonparametric supervised learning method that builds a binary decision tree for classification tasks. At each step, the algorithm selects a feature and threshold that create two child nodes with lower class impurity, using criteria such as Gini impurity or entropy. Through this recursive partitioning, CART represents the classifier as a set of piecewise-constant regions, where each leaf corresponds to a predicted class label. Because the sequence of splits directly mirrors the decision-making process, CART offers a transparent and intuitive model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf3270",
   "metadata": {},
   "source": [
    "#### 0.2 Advantages  \n",
    "CART offers several notable strengths that contribute to its widespread use as a baseline classifier. First, the model is highly interpretable: each internal node corresponds to a clear “if–then” condition based on a single feature, allowing the entire decision path to be easily traced and communicated. This transparency is particularly valuable in settings where model explanations are required.  \n",
    "\n",
    "Second, CART is able to capture nonlinear relationships and feature interactions without relying on explicit transformations or parametric assumptions. Its recursive splitting procedure enables the model to adapt flexibly to irregular or complex decision boundaries, providing expressive power beyond that of linear models.  \n",
    "\n",
    "Moreover, CART requires minimal preprocessing. It can accommodate both numerical and categorical variables, is robust to monotonic feature scaling, and implicitly performs feature selection by choosing splits only on informative variables. These characteristics make CART convenient to implement and reliable across a wide range of practical applications.\n",
    "\n",
    "#### 0.3 Disadvantages  \n",
    "Despite its advantages, CART also presents several limitations that must be considered. Most importantly, the model is prone to overfitting when allowed to grow without constraints. As emphasized in the bias–complexity trade-off discussed in the course reading, increasing model flexibility reduces approximation error but raises estimation error, causing deep, unpruned trees to exhibit high variance and poor generalization.  \n",
    "\n",
    "CART also tends to be unstable: small perturbations in the training data can alter early splits, resulting in substantially different tree structures. This sensitivity undermines the model’s reliability, especially in contexts requiring stable predictions.  \n",
    "\n",
    "Finally, because CART relies exclusively on axis-aligned splits, it may need many successive partitions to approximate diagonal or curved decision boundaries, leading to unnecessarily deep and complex trees. These shortcomings motivate the use of pruning techniques and more advanced ensemble methods, such as Random Forests and Gradient Boosting, which address variance and stability issues more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81ff2",
   "metadata": {},
   "source": [
    "### 1. Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd5a5e-0718-439a-8ef4-f6edf3b93136",
   "metadata": {},
   "source": [
    "#### 1.1 **Domain Set**\n",
    "We define the domain space as  \n",
    "\n",
    "In the CART classification setting, each training example is represented as a feature vector in an $n$-dimensional real space:\n",
    "\n",
    "$$\n",
    "\\mathcal{X} = \\mathbb{R}^n, \\qquad x_i = (x_{i1}, x_{i2}, \\dots, x_{in}) \\in \\mathcal{X}.\n",
    "$$\n",
    "\n",
    "Each component $x_{ij}$ represents the value of feature $j$ for sample $i$.\n",
    "The feature domain can include continuous or categorical variables (encoded numerically in practice).\n",
    "\n",
    "\n",
    "\n",
    "#### 1.2 **Label Set**\n",
    "\n",
    "For a $K$-class classification task, the label space is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{Y} = \\{0, 1, \\dots, K-1\\}.\n",
    "$$\n",
    "\n",
    "In the binary case, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathcal{Y} = \\{0, 1\\}.\n",
    "$$\n",
    "\n",
    "#### 1.3 **Training Data**\n",
    "\n",
    "We are given a labeled dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N},\n",
    "\\quad x_i \\in \\mathcal{X}, \\; y_i \\in \\mathcal{Y}.\n",
    "$$\n",
    "\n",
    "Each pair $(x_i, y_i)$ represents one training example.\n",
    "The training process recursively partitions $\\mathcal{D}$ based on feature thresholds to form a binary decision tree.\n",
    "\n",
    "#### 1.4 **Learner's Output**\n",
    "Formally, the hypothesis space of CART classification is defined as the set of **binary decision trees** of depth at most $T_{\\max}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{H} =\n",
    "\\{\\, h : \\mathcal{X} \\to \\mathcal{Y} \\mid\n",
    "h \\text{ is a binary decision tree with depth } \\le T_{\\max} \\,\\}.\n",
    "$$\n",
    "\n",
    "Each decision tree $h \\in \\mathcal{H}$ recursively partitions the input space $\\mathcal{X}$ into at most $2^{T_{\\max}}$ disjoint leaves.\n",
    "\n",
    "At prediction time, a new observation $x$ is passed through the sequence of feature tests $(x_f \\le t)$ until it reaches a leaf node $i$.\n",
    "Each leaf stores an empirical class probability vector\n",
    "\n",
    "$$\n",
    "p_i = (p_{i,0}, p_{i,1}, \\dots, p_{i,K-1}),\n",
    "$$\n",
    "\n",
    "computed from the training samples that reached that leaf.\n",
    "\n",
    "The predicted class label is then determined by\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\arg\\max_k \\, p_{i,k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc5ddd",
   "metadata": {},
   "source": [
    "### 2. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab2653",
   "metadata": {},
   "source": [
    "In the classification setting, losses are the **mearuses of impurity**.  CART minimizes impruity and the loss is defined per split. Generally speaking, **Gini** and **Entropy** are good measures.\n",
    "\n",
    "To compute Loss, we need: \n",
    "* Impurity measure, \n",
    "* Split loss based on choosen impurity measure.\n",
    "\n",
    "In the scikit-learn, this is determined by the parameter **criterion**: *{“gini”, “entropy”, “log_loss”}, default=”gini”* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e29c6",
   "metadata": {},
   "source": [
    "#### 2.1 **Impurity Function**\n",
    "\n",
    "For a $K$-class classification problem, consider node $i$ containing a subset of samples\n",
    "\n",
    "$$S_i = \\{(x_j, y_j)\\}_{j \\in \\mathcal{I}_i}, \\qquad N_i = |S_i|.$$\n",
    "\n",
    "The number of samples in node $i$ that belong to class $k$ is\n",
    "\n",
    "$$n_{i,k} = \\sum_{j \\in \\mathcal{I}_i} \\mathbf{1}(y_j = k).$$\n",
    "\n",
    "The class proportion of class $k$ in node $i$ is\n",
    "\n",
    "$$p_{i,k} = \\frac{n_{i,k}}{N_i}, \\qquad k = 1, \\dots, K.$$\n",
    "$$\\sum_{k=1}^K p_{i,k} = 1,\\text{and  } p_{i,k} \\ge 0 \\quad \\text{for } k = 1, \\dots, K.$$\n",
    "\n",
    "##### 2.1.1 **Gini**\n",
    "\n",
    "\n",
    "- The Gini impurity of node $i$ is:   \n",
    "$$G_i = 1 - \\sum_{k=1}^K p_{i,k}^2.$$\n",
    "\n",
    "##### 2.1.2 **Entropy**\n",
    "\n",
    "- The entropy impurity of node $i$ is\n",
    "$$H_i = - \\sum_{k=1}^K p_{i,k} \\log p_{i,k},$$\n",
    "\n",
    "- And we assume $0 \\log 0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad0f55",
   "metadata": {},
   "source": [
    "#### 2.2 **Split Loss**\n",
    "\n",
    "Given a candidate split $\\theta$ applied at node $i$, the dataset $S_i$ is partitioned into a left subset $S_i^{\\text{left}}(\\theta)$ and a right subset $S_i^{\\text{right}}(\\theta)$:\n",
    "\n",
    "$$\n",
    "S_i^{\\text{left}}(\\theta) = \\{(x_j, y_j) \\in S_i \\mid x_{j, f} \\le t\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_i^{\\text{right}}(\\theta) = S_i \\setminus S_i^{\\text{left}}(\\theta),\n",
    "$$\n",
    "\n",
    "where $\\theta = (f, t)$ denotes the split feature index $f$ and the threshold value $t$.\n",
    "\n",
    "Let the number of samples in the left and right subsets be\n",
    "\n",
    "$$\n",
    "N_i^{\\text{left}} = |S_i^{\\text{left}}(\\theta)|, \\qquad \n",
    "N_i^{\\text{right}} = |S_i^{\\text{right}}(\\theta)|.\n",
    "$$\n",
    "\n",
    "Their corresponding class proportions are computed in the same way as in Section 2.1.\n",
    "\n",
    "\n",
    "##### 2.2.1 **Weighted Child Impurity**\n",
    "\n",
    "Given an impurity function $C(\\cdot)$ (e.g., Gini or entropy), the **split loss** at node $i$ for candidate split $\\theta$ is defined as the weighted sum of the left and right child impurities:\n",
    "\n",
    "$$\n",
    "L(S_i, \\theta) \n",
    "= \n",
    "\\frac{N_i^{\\text{left}}}{N_i} \n",
    "\\, C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)\n",
    "\\;+\\;\n",
    "\\frac{N_i^{\\text{right}}}{N_i}\n",
    "\\, C\\!\\left(S_i^{\\text{right}}(\\theta)\\right).\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $C\\!\\left(S_i^{\\text{left}}(\\theta)\\right)$ is the impurity (Gini or entropy) of the left child node.\n",
    "- $C\\!\\left(S_i^{\\text{right}}(\\theta)\\right)$ is the impurity of the right child node.\n",
    "\n",
    "\n",
    "##### 2.2.2 **Optimal Split Selection**\n",
    "\n",
    "The optimal split parameter is chosen by minimizing the split loss:\n",
    "\n",
    "$$\n",
    "\\theta^{*} = \\arg\\min_{\\theta} \\; L(S_i, \\theta).\n",
    "$$\n",
    "\n",
    "And this will be futher explained in the next part, Optimizer on how to actually implement it.\n",
    "\n",
    "- ***Reference***: scikit-learn mathematical formulation https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eed380",
   "metadata": {},
   "source": [
    "### 3. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71bbe77",
   "metadata": {},
   "source": [
    "### 3.1 What is Optimized in CART\n",
    "\n",
    "CART performs a **greedy, recursive partitioning** - at each node, it selects the best split that maximizes information gain (or equivalently minimizes impurity).\n",
    "\n",
    "So the optimizer is essentially a **greedy search algorithm** that finds:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{(f,t)} \\; \\text{Impurity}(S_{\\text{left}}) + \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "where $f$ is the feature and $t$ is the threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d074bd",
   "metadata": {},
   "source": [
    "#### 3.1.1 Objective Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c847f",
   "metadata": {},
   "source": [
    "CART minimizes an **impurity measure** (loss function) such as:\n",
    "- Gini Index:\n",
    "$$ G(S) = 1 - \\sum_{k=1}^{K}p_k^2 $$\n",
    "- Entropy:\n",
    "$$ H(S) = - \\sum_{k=1}^{K}p_klog(p_k)$$\n",
    "\n",
    "At each node:\n",
    "$$\n",
    "\\text{Gain}(S, f, t) = \\text{Impurity}(S) \n",
    "- \\frac{|S_{\\text{left}}|}{|S|} \\, \\text{Impurity}(S_{\\text{left}}) \n",
    "- \\frac{|S_{\\text{right}}|}{|S|} \\, \\text{Impurity}(S_{\\text{right}})\n",
    "$$\n",
    "\n",
    "The algorithm chooses the feature $f*$ and threshold $t*$ that maximize this gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c31b7",
   "metadata": {},
   "source": [
    "#### 3.1.2 Pseudo-code\n",
    "\n",
    "Intuitively, the algorithm asks: “Which feature and cutoff most cleanly separates the classes?” By evaluating all possible splits and picking the one that reduces impurity the most, CART greedily chooses the single question that best organizes the data at this point in the tree. This local optimization step is repeated recursively to grow the whole decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0b67a",
   "metadata": {},
   "source": [
    "```python\n",
    "Inputs: dataset S, feature set F, impurity measure Impurity()\n",
    "\n",
    "best_gain ← 0  \n",
    "best_feature, best_threshold ← None  \n",
    "\n",
    "for each feature f in F:  \n",
    " for each possible threshold t in f:  \n",
    "  Split S into S_left and S_right using (f, t)  \n",
    "  if either split is empty: continue  \n",
    "  gain ← Impurity(S) \n",
    "     - (|S_left| / |S|) * Impurity(S_left)\n",
    "     - (|S_right| / |S|) * Impurity(S_right)  \n",
    "  if gain > best_gain:  \n",
    "   best_gain ← gain  \n",
    "   best_feature ← f  \n",
    "   best_threshold ← t  \n",
    "\n",
    "return (best_feature, best_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f366918",
   "metadata": {},
   "source": [
    "This pseudocode describes how CART chooses the best split at a node by searching over all features and all possible thresholds. It begins by initializing best_gain and empty placeholders for the best split. For each feature, the algorithm tries every potential threshold and divides the dataset into **S_left** and **S_right**. If the split is invalid (one side is empty), it skips it. Otherwise, it computes the **impurity reduction** (gain): the impurity of the parent minus the weighted impurities of the two child subsets. If this gain is better than any previous one, the algorithm updates the best feature and threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c447cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
